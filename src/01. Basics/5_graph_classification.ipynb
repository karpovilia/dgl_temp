{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import math\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.0.1+cu117.  CUDA version: 11.7\n"
     ]
    }
   ],
   "source": [
    "print(f\"Torch version: {th.__version__}.  CUDA version: {th.version.cuda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['DGLBACKEND'] = 'pytorch'\n",
    "import dgl\n",
    "from dgl.data import DGLDataset\n",
    "from dgl.dataloading import GraphDataLoader\n",
    "from torch.utils.data import Subset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "import dgl.function as fn\n",
    "\n",
    "from ogb.graphproppred import DglGraphPropPredDataset, Evaluator\n",
    "from ogb.graphproppred.mol_encoder import BondEncoder, AtomEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.9.1post1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dgl.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if th.cuda.is_available() else 'cpu'\n",
    "device = th.device(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import repeat_experiments, norm_plot, train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.dgl.ai/tutorials/blitz/6_load_data.html#creating-a-dataset-for-graph-classification-from-csv\n",
    "class MolHIVDataset(DGLDataset):\n",
    "    def __init__(self):\n",
    "        super().__init__(name='ogbg-molhiv')\n",
    "        \n",
    "    def _load(self):\n",
    "        self.dataset = DglGraphPropPredDataset(name=\"ogbg-molhiv\", root = 'dataset/')\n",
    "        self.split_idx = self.dataset.get_idx_split()\n",
    "        print(self.dataset.meta_info)\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        return self.dataset.graphs[i], self.dataset.labels[i]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset.graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num tasks                                                                1\n",
      "eval metric                                                         rocauc\n",
      "download_name                                                          hiv\n",
      "version                                                                  1\n",
      "url                      http://snap.stanford.edu/ogb/data/graphproppre...\n",
      "add_inverse_edge                                                      True\n",
      "data type                                                              mol\n",
      "has_node_attr                                                         True\n",
      "has_edge_attr                                                         True\n",
      "task type                                            binary classification\n",
      "num classes                                                              2\n",
      "split                                                             scaffold\n",
      "additional node files                                                 None\n",
      "additional edge files                                                 None\n",
      "binary                                                               False\n",
      "Name: ogbg-molhiv, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "41127"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = MolHIVDataset()\n",
    "len(dataset) #41127"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train/val/test split dataloaders\n",
    "split_idx = dataset.split_idx\n",
    "\n",
    "train_sampler = SubsetRandomSampler(split_idx['train'])\n",
    "val_subset = Subset(dataset, split_idx['valid'])\n",
    "test_subset = Subset(dataset, split_idx['test'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 32\n",
    "train_dataloader = GraphDataLoader(\n",
    "    dataset, sampler=train_sampler, batch_size=bs, drop_last=False)\n",
    "val_dataloader = GraphDataLoader(\n",
    "    val_subset, shuffle=False, batch_size=bs, drop_last=False)\n",
    "test_dataloader = GraphDataLoader(\n",
    "    test_subset, shuffle=False, batch_size=bs, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4113, 4113)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_subset), len(test_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(train_dataloader)\n",
    "batch = next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Graph(num_nodes=706, num_edges=1512,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)}),\n",
       " tensor([[0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0]])]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Graph(num_nodes=22, num_edges=48,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)}),\n",
       " Graph(num_nodes=25, num_edges=52,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)}),\n",
       " Graph(num_nodes=20, num_edges=40,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)}),\n",
       " Graph(num_nodes=23, num_edges=52,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)}),\n",
       " Graph(num_nodes=23, num_edges=50,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)}),\n",
       " Graph(num_nodes=11, num_edges=20,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)}),\n",
       " Graph(num_nodes=36, num_edges=78,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)}),\n",
       " Graph(num_nodes=14, num_edges=32,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)}),\n",
       " Graph(num_nodes=18, num_edges=36,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)}),\n",
       " Graph(num_nodes=27, num_edges=60,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)}),\n",
       " Graph(num_nodes=12, num_edges=20,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)}),\n",
       " Graph(num_nodes=22, num_edges=50,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)}),\n",
       " Graph(num_nodes=35, num_edges=76,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)}),\n",
       " Graph(num_nodes=38, num_edges=84,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)}),\n",
       " Graph(num_nodes=21, num_edges=44,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)}),\n",
       " Graph(num_nodes=17, num_edges=36,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)}),\n",
       " Graph(num_nodes=21, num_edges=44,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)}),\n",
       " Graph(num_nodes=17, num_edges=36,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)}),\n",
       " Graph(num_nodes=12, num_edges=26,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)}),\n",
       " Graph(num_nodes=32, num_edges=72,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)}),\n",
       " Graph(num_nodes=24, num_edges=50,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)}),\n",
       " Graph(num_nodes=20, num_edges=46,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)}),\n",
       " Graph(num_nodes=26, num_edges=58,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)}),\n",
       " Graph(num_nodes=29, num_edges=64,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)}),\n",
       " Graph(num_nodes=35, num_edges=72,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)}),\n",
       " Graph(num_nodes=19, num_edges=40,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)}),\n",
       " Graph(num_nodes=14, num_edges=30,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)}),\n",
       " Graph(num_nodes=24, num_edges=52,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)}),\n",
       " Graph(num_nodes=15, num_edges=30,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)}),\n",
       " Graph(num_nodes=23, num_edges=48,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)}),\n",
       " Graph(num_nodes=19, num_edges=42,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)}),\n",
       " Graph(num_nodes=12, num_edges=24,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)})]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dgl.unbatch(batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[7, 0, 2,  ..., 2, 0, 0],\n",
       "        [5, 0, 4,  ..., 2, 0, 0],\n",
       "        [5, 0, 3,  ..., 1, 1, 1],\n",
       "        ...,\n",
       "        [6, 0, 3,  ..., 1, 1, 1],\n",
       "        [5, 0, 3,  ..., 1, 1, 1],\n",
       "        [7, 0, 1,  ..., 1, 0, 0]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ndata - node data\n",
    "batch[0].ndata['feat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [1, 0, 1],\n",
       "        [3, 0, 1],\n",
       "        [3, 0, 1]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# edata - edge data\n",
    "batch[0].edata['feat']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see [AtomEncoder](https://github.com/snap-stanford/ogb/blob/68a303f320220cda859e83e3a8660f2b9debedf6/ogb/graphproppred/mol_encoder.py#L7) and [BondEncoder](https://github.com/snap-stanford/ogb/blob/68a303f320220cda859e83e3a8660f2b9debedf6/ogb/graphproppred/mol_encoder.py#L27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Three parts:\n",
    "## GIN layer\n",
    "## Node convolution\n",
    "## Graph convolution"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Isomorphic Network layer\n",
    "\n",
    "[How Powerful are Graph Neural Networks?](https://arxiv.org/abs/1810.00826)\n",
    "\n",
    "$$\\mathbf{h}_i^{(l+1)} = \\text{MLP}[ (1 + \\epsilon) \\cdot \\mathbf{h}_i^{(l)} + \\sum_{j \\in \\mathcal{N}_i} \\mathbf{h}_j^{(l)} ]$$\n",
    "\n",
    "see https://github.com/snap-stanford/ogb/blob/master/examples/graphproppred/mol/conv.py#L31\n",
    "\n",
    "$$\\mathbf{h}_i^{(l+1)} = \\text{MLP}[ (1 + \\epsilon) \\cdot \\mathbf{h}_i^{(l)} + \\sum_{j \\in \\mathcal{N}_i} \\text{ReLU}(\\mathbf{h}_j^{(l)} + \\mathbf{e}_{ij}) ]$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy_{u} - source node\n",
    "# copy_{v} - destination, neighbour node\n",
    "\n",
    "class GINLayer(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super(GINLayer, self).__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(emb_dim, 2*emb_dim), \n",
    "            nn.BatchNorm1d(2*emb_dim), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(2*emb_dim, emb_dim) #from above\n",
    "        )\n",
    "        self.eps = nn.Parameter(th.FloatTensor([0]))\n",
    "        \n",
    "\n",
    "    def reset_parameters(self): # reset the parameters of the MLP\n",
    "        # MLP\n",
    "        for layer in self.mlp:\n",
    "            if hasattr(layer, 'reset_parameters'):\n",
    "                layer.reset_parameters()\n",
    "        # EPS\n",
    "        nn.init.constant_(self.eps, 0.)\n",
    "        \n",
    "    def extra_repr(self):\n",
    "        # Add \"eps\" to the string representation\n",
    "        return f'(eps): nn.Parameter'\n",
    "        \n",
    "    def forward(self, g, x_node, x_edge):\n",
    "        \"\"\"\n",
    "        g : The graph used for message passing\n",
    "        x_node : AtomEncodings\n",
    "        x_edge : BondEncodings\n",
    "        \"\"\"\n",
    "        \n",
    "        with g.local_scope():\n",
    "            # Store edge features to 'bond' key in g.edata\n",
    "            g.edata['bond'] = x_edge\n",
    "            # Store node features to 'x' key in g.ndata\n",
    "            g.srcdata['x'] = x_node\n",
    "            \n",
    "            # calculate the right part h_i = sum_j( F.relu (h_j + e_ij) )\n",
    "                \n",
    "            ## 1. take the sum\n",
    "            g.apply_edges( # see https://docs.dgl.ai/api/python/dgl.function.html#dgl-built-in-function\n",
    "                fn.u_add_e('x', 'bond', 'm')\n",
    "            )\n",
    "            ##2. apply ReLU\n",
    "            g.edata['m'] = F.relu(g.edata['m'])\n",
    "            \n",
    "            ## 3. sum the modified messages https://docs.dgl.ai/en/1.0.x/generated/dgl.DGLGraph.update_all.html\n",
    "            g.update_all(\n",
    "                fn.copy_e('m', 'm'), # creating messages\n",
    "                fn.sum('m', 'mp'), # sending messages to message passing\n",
    "            )\n",
    "            ## extract the final output into variable \"h_mp\"\n",
    "            h_mp = g.dstdata['mp'] # will be used in the next layer\n",
    "                    \n",
    "        # GIN update equation\n",
    "        out = self.mlp((1 + self.eps) * x_node + h_mp)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GINLayer(\n",
       "  (eps): nn.Parameter\n",
       "  (mlp): Sequential(\n",
       "    (0): Linear(in_features=300, out_features=600, bias=True)\n",
       "    (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=600, out_features=300, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gin = GINLayer(emb_dim)\n",
    "gin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "362101"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#362101\n",
    "sum(p.numel() for p in gin.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ki/repos/dgl2/venv/lib/python3.10/site-packages/dgl/backend/pytorch/tensor.py:352: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  assert input.numel() == input.storage().size(), \"Cannot convert view \" \\\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9681, -0.6240, -0.1038,  ..., -0.6000, -0.9485, -0.7573],\n",
       "        [ 0.3010, -0.6838,  0.5443,  ..., -0.0686, -0.7225, -0.6984],\n",
       "        [ 0.0730,  0.0048, -0.0595,  ..., -0.2102, -0.4386, -0.5446],\n",
       "        ...,\n",
       "        [ 0.4252,  0.1886, -0.2115,  ..., -0.2981, -0.3695, -0.0432],\n",
       "        [ 0.1600, -0.1270, -0.0809,  ..., -0.1951, -0.2563, -0.3816],\n",
       "        [-0.1221, -0.7057,  0.3895,  ..., -0.7057, -0.6778, -0.4727]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://github.com/dmlc/dgl/blob/master/examples/mxnet/gin/gin.py\n",
    "\n",
    "gin(\n",
    "    batch[0], \n",
    "    AtomEncoder(emb_dim)(batch[0].ndata['feat']), \n",
    "    BondEncoder(emb_dim)(batch[0].edata['feat'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Node convolution\n",
    "class NodeGNN(nn.Module):\n",
    "    def __init__(self, emb_dim, num_layers, dropout):\n",
    "        super(NodeGNN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.atom_encoder = AtomEncoder(emb_dim)\n",
    "        self.bond_encoder = BondEncoder(emb_dim)\n",
    "        \n",
    "        self.convs = nn.ModuleList()\n",
    "        self.batch_norms = nn.ModuleList()\n",
    "        for layer in range(num_layers):\n",
    "            self.convs.append(GINLayer(emb_dim))\n",
    "            self.batch_norms.append(nn.BatchNorm1d(emb_dim))\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        \n",
    "        for emb in self.atom_encoder.atom_embedding_list: # Atom embeddings\n",
    "            nn.init.xavier_uniform_(emb.weight.data)\n",
    "        \n",
    "        for emb in self.bond_encoder.bond_embedding_list: # Bond embeddings\n",
    "            nn.init.xavier_uniform_(emb.weight.data)\n",
    "        \n",
    "        # reset parameters for the GIN and batch norm layers\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "        for bn in self.batch_norms:\n",
    "            bn.reset_parameters()\n",
    "    \n",
    "    def forward(self, g):\n",
    "        \"\"\"        \n",
    "        (node_embeddings, edge_embeddings) -> GIN -> Batch Norm -> ReLU -> Dropout\n",
    "        \"\"\"\n",
    "        \n",
    "        # Convert integer categorical features to embeddings\n",
    "        h = self.atom_encoder(g.ndata['feat']) # initial node embeddings\n",
    "        edge_embedding = self.bond_encoder(g.edata['feat']) # edge embeddings\n",
    "        \n",
    "        for layer in range(self.num_layers):\n",
    "            h = self.convs[layer](g, h, edge_embedding)\n",
    "            h = self.batch_norms[layer](h)\n",
    "            if layer == self.num_layers - 1:\n",
    "                #remove relu for the last layer\n",
    "                h = F.dropout(h, self.dropout, training=self.training)\n",
    "            else:\n",
    "                h = F.dropout(F.relu(h), self.dropout, training=self.training)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NodeGNN(\n",
       "  (atom_encoder): AtomEncoder(\n",
       "    (atom_embedding_list): ModuleList(\n",
       "      (0): Embedding(119, 300)\n",
       "      (1): Embedding(5, 300)\n",
       "      (2-3): 2 x Embedding(12, 300)\n",
       "      (4): Embedding(10, 300)\n",
       "      (5-6): 2 x Embedding(6, 300)\n",
       "      (7-8): 2 x Embedding(2, 300)\n",
       "    )\n",
       "  )\n",
       "  (bond_encoder): BondEncoder(\n",
       "    (bond_embedding_list): ModuleList(\n",
       "      (0): Embedding(5, 300)\n",
       "      (1): Embedding(6, 300)\n",
       "      (2): Embedding(2, 300)\n",
       "    )\n",
       "  )\n",
       "  (convs): ModuleList(\n",
       "    (0): GINLayer(\n",
       "      (eps): nn.Parameter\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=300, out_features=600, bias=True)\n",
       "        (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Linear(in_features=600, out_features=300, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (batch_norms): ModuleList(\n",
       "    (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test model\n",
    "node_gnn = NodeGNN(emb_dim, 1, 0.5)\n",
    "node_gnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "418801"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#418501\n",
    "sum(p.numel() for p in node_gnn.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000, -0.3458, -2.4443,  ...,  0.0000, -1.9417,  0.0000],\n",
       "        [-0.0000, -0.0000, -0.0000,  ...,  0.0000,  0.0000, -1.2409],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ..., -0.5370, -0.0000, -0.5971],\n",
       "        ...,\n",
       "        [-0.0000, -0.0000, -2.1739,  ..., -0.3948,  3.4321,  0.4168],\n",
       "        [-0.0000, -0.0000, -0.0000,  ..., -1.7384, -2.3997,  1.1329],\n",
       "        [-0.0000, -0.5160,  0.0962,  ...,  0.0000,  0.0000, -0.0000]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_gnn(batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph convolution\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mathbf{h}_\\mathcal{G} = \\frac{1}{|V_\\mathcal{G}|}\\sum_{j \\in V_\\mathcal{G}} \\mathbf{h}_j$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphGNN(nn.Module):\n",
    "    def __init__(self, emb_dim, num_layers, node_cls, dropout):\n",
    "        super(GraphGNN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.node_GNN = node_cls(emb_dim, num_layers, dropout) # node-level GNN, returns node embeddings for every single graph\n",
    "        #node representation -> graph representation\n",
    "        self.graph_pred_linear = nn.Linear(emb_dim, 1)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.node_GNN.reset_parameters()\n",
    "        self.graph_pred_linear.reset_parameters()\n",
    "\n",
    "\n",
    "    def forward(self, g): # !!! g is a batched super-graph\n",
    "        h_node = self.node_GNN(g) # node-level embeddings with self.node_GNN\n",
    "        \n",
    "        # pool the node-level embedding to get graph representations\n",
    "        with g.local_scope():\n",
    "            g.ndata['h'] = h_node # store node data in h\n",
    "            h_graph = dgl.mean_nodes(g, 'h') # average the nodes in the component graphs 32 graph layer embs\n",
    "        \n",
    "        # generate a final prediction by sending the graph-level representation\n",
    "        # through self.graph_pred_linear\n",
    "        pred = self.graph_pred_linear(h_graph)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 5\n",
    "dropout = 0.5\n",
    "\n",
    "model = GraphGNN(emb_dim, num_layers, NodeGNN, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphGNN(\n",
       "  (node_GNN): NodeGNN(\n",
       "    (atom_encoder): AtomEncoder(\n",
       "      (atom_embedding_list): ModuleList(\n",
       "        (0): Embedding(119, 300)\n",
       "        (1): Embedding(5, 300)\n",
       "        (2-3): 2 x Embedding(12, 300)\n",
       "        (4): Embedding(10, 300)\n",
       "        (5-6): 2 x Embedding(6, 300)\n",
       "        (7-8): 2 x Embedding(2, 300)\n",
       "      )\n",
       "    )\n",
       "    (bond_encoder): BondEncoder(\n",
       "      (bond_embedding_list): ModuleList(\n",
       "        (0): Embedding(5, 300)\n",
       "        (1): Embedding(6, 300)\n",
       "        (2): Embedding(2, 300)\n",
       "      )\n",
       "    )\n",
       "    (convs): ModuleList(\n",
       "      (0-4): 5 x GINLayer(\n",
       "        (eps): nn.Parameter\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=300, out_features=600, bias=True)\n",
       "          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "          (3): Linear(in_features=600, out_features=300, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (batch_norms): ModuleList(\n",
       "      (0-4): 5 x BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1869906"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1869606\n",
    "sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_runs = 10\n",
    "train_args = dict(epochs=100, lr=0.001, eval_steps=1, log_steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 01, Epoch: 01, Loss: 0.1936, Train: 0.7169 AUC, Valid: 0.6502 AUC, Test: 0.6184 AUC\n",
      "---\n",
      "Run: 01, Epoch: 02, Loss: 0.1511, Train: 0.7228 AUC, Valid: 0.6945 AUC, Test: 0.6963 AUC\n",
      "---\n",
      "Run: 01, Epoch: 03, Loss: 0.1477, Train: 0.7572 AUC, Valid: 0.7510 AUC, Test: 0.7142 AUC\n",
      "---\n",
      "Run: 01, Epoch: 04, Loss: 0.1458, Train: 0.7318 AUC, Valid: 0.7256 AUC, Test: 0.7532 AUC\n",
      "---\n",
      "Run: 01, Epoch: 05, Loss: 0.1433, Train: 0.7547 AUC, Valid: 0.7528 AUC, Test: 0.7468 AUC\n",
      "---\n",
      "Run: 01, Epoch: 06, Loss: 0.1420, Train: 0.7616 AUC, Valid: 0.7648 AUC, Test: 0.6973 AUC\n",
      "---\n",
      "Run: 01, Epoch: 07, Loss: 0.1413, Train: 0.7860 AUC, Valid: 0.7818 AUC, Test: 0.7124 AUC\n",
      "---\n",
      "Run: 01, Epoch: 08, Loss: 0.1389, Train: 0.7822 AUC, Valid: 0.7881 AUC, Test: 0.7458 AUC\n",
      "---\n",
      "Run: 01, Epoch: 09, Loss: 0.1370, Train: 0.7683 AUC, Valid: 0.7512 AUC, Test: 0.7417 AUC\n",
      "---\n",
      "Run: 01, Epoch: 10, Loss: 0.1361, Train: 0.7930 AUC, Valid: 0.7913 AUC, Test: 0.7369 AUC\n",
      "---\n",
      "Run: 01, Epoch: 11, Loss: 0.1361, Train: 0.7796 AUC, Valid: 0.7827 AUC, Test: 0.7130 AUC\n",
      "---\n",
      "Run: 01, Epoch: 12, Loss: 0.1349, Train: 0.8006 AUC, Valid: 0.7864 AUC, Test: 0.7323 AUC\n",
      "---\n",
      "Run: 01, Epoch: 13, Loss: 0.1346, Train: 0.8092 AUC, Valid: 0.7831 AUC, Test: 0.7650 AUC\n",
      "---\n",
      "Run: 01, Epoch: 14, Loss: 0.1321, Train: 0.8068 AUC, Valid: 0.8034 AUC, Test: 0.7501 AUC\n",
      "---\n",
      "Run: 01, Epoch: 15, Loss: 0.1302, Train: 0.8035 AUC, Valid: 0.7902 AUC, Test: 0.7327 AUC\n",
      "---\n",
      "Run: 01, Epoch: 16, Loss: 0.1293, Train: 0.8191 AUC, Valid: 0.7987 AUC, Test: 0.7713 AUC\n",
      "---\n",
      "Run: 01, Epoch: 17, Loss: 0.1278, Train: 0.8153 AUC, Valid: 0.7935 AUC, Test: 0.7492 AUC\n",
      "---\n",
      "Run: 01, Epoch: 18, Loss: 0.1274, Train: 0.8172 AUC, Valid: 0.7754 AUC, Test: 0.7420 AUC\n",
      "---\n",
      "Run: 01, Epoch: 19, Loss: 0.1255, Train: 0.8285 AUC, Valid: 0.8011 AUC, Test: 0.7397 AUC\n",
      "---\n",
      "Run: 01, Epoch: 20, Loss: 0.1259, Train: 0.8265 AUC, Valid: 0.7921 AUC, Test: 0.7469 AUC\n",
      "---\n",
      "Run: 01, Epoch: 21, Loss: 0.1242, Train: 0.8306 AUC, Valid: 0.7923 AUC, Test: 0.7369 AUC\n",
      "---\n",
      "Run: 01, Epoch: 22, Loss: 0.1245, Train: 0.8428 AUC, Valid: 0.8132 AUC, Test: 0.7356 AUC\n",
      "---\n",
      "Run: 01, Epoch: 23, Loss: 0.1219, Train: 0.8479 AUC, Valid: 0.8091 AUC, Test: 0.7420 AUC\n",
      "---\n",
      "Run: 01, Epoch: 24, Loss: 0.1234, Train: 0.8493 AUC, Valid: 0.8176 AUC, Test: 0.7407 AUC\n",
      "---\n",
      "Run: 01, Epoch: 25, Loss: 0.1221, Train: 0.8501 AUC, Valid: 0.8036 AUC, Test: 0.7287 AUC\n",
      "---\n",
      "Run: 01, Epoch: 26, Loss: 0.1206, Train: 0.8549 AUC, Valid: 0.8093 AUC, Test: 0.7543 AUC\n",
      "---\n",
      "Run: 01, Epoch: 27, Loss: 0.1205, Train: 0.8507 AUC, Valid: 0.8280 AUC, Test: 0.7313 AUC\n",
      "---\n",
      "Run: 01, Epoch: 28, Loss: 0.1196, Train: 0.8416 AUC, Valid: 0.8047 AUC, Test: 0.7536 AUC\n",
      "---\n",
      "Run: 01, Epoch: 29, Loss: 0.1182, Train: 0.8554 AUC, Valid: 0.7959 AUC, Test: 0.7521 AUC\n",
      "---\n",
      "Run: 01, Epoch: 30, Loss: 0.1178, Train: 0.8562 AUC, Valid: 0.7726 AUC, Test: 0.7487 AUC\n",
      "---\n",
      "Run: 01, Epoch: 31, Loss: 0.1163, Train: 0.8686 AUC, Valid: 0.8138 AUC, Test: 0.7532 AUC\n",
      "---\n",
      "Run: 01, Epoch: 32, Loss: 0.1162, Train: 0.8655 AUC, Valid: 0.8022 AUC, Test: 0.7486 AUC\n",
      "---\n",
      "Run: 01, Epoch: 33, Loss: 0.1164, Train: 0.8714 AUC, Valid: 0.8186 AUC, Test: 0.7459 AUC\n",
      "---\n",
      "Run: 01, Epoch: 34, Loss: 0.1144, Train: 0.8698 AUC, Valid: 0.8019 AUC, Test: 0.7512 AUC\n",
      "---\n",
      "Run: 01, Epoch: 35, Loss: 0.1155, Train: 0.8761 AUC, Valid: 0.8129 AUC, Test: 0.7489 AUC\n",
      "---\n",
      "Run: 01, Epoch: 36, Loss: 0.1142, Train: 0.8807 AUC, Valid: 0.7875 AUC, Test: 0.7290 AUC\n",
      "---\n",
      "Run: 01, Epoch: 37, Loss: 0.1138, Train: 0.8754 AUC, Valid: 0.7933 AUC, Test: 0.7433 AUC\n",
      "---\n",
      "Run: 01, Epoch: 38, Loss: 0.1132, Train: 0.8871 AUC, Valid: 0.8029 AUC, Test: 0.7462 AUC\n",
      "---\n",
      "Run: 01, Epoch: 39, Loss: 0.1121, Train: 0.8835 AUC, Valid: 0.7928 AUC, Test: 0.7427 AUC\n",
      "---\n",
      "Run: 01, Epoch: 40, Loss: 0.1129, Train: 0.8896 AUC, Valid: 0.7977 AUC, Test: 0.7508 AUC\n",
      "---\n",
      "Run: 01, Epoch: 41, Loss: 0.1100, Train: 0.8782 AUC, Valid: 0.8174 AUC, Test: 0.7455 AUC\n",
      "---\n",
      "Run: 01, Epoch: 42, Loss: 0.1116, Train: 0.8850 AUC, Valid: 0.8121 AUC, Test: 0.7487 AUC\n",
      "---\n",
      "Run: 01, Epoch: 43, Loss: 0.1095, Train: 0.8889 AUC, Valid: 0.8152 AUC, Test: 0.7479 AUC\n",
      "---\n",
      "Run: 01, Epoch: 44, Loss: 0.1095, Train: 0.8891 AUC, Valid: 0.7882 AUC, Test: 0.7515 AUC\n",
      "---\n",
      "Run: 01, Epoch: 45, Loss: 0.1091, Train: 0.8974 AUC, Valid: 0.8114 AUC, Test: 0.7668 AUC\n",
      "---\n",
      "Run: 01, Epoch: 46, Loss: 0.1085, Train: 0.8963 AUC, Valid: 0.7816 AUC, Test: 0.7497 AUC\n",
      "---\n",
      "Run: 01, Epoch: 47, Loss: 0.1086, Train: 0.8993 AUC, Valid: 0.7963 AUC, Test: 0.7430 AUC\n",
      "---\n",
      "Run: 01, Epoch: 48, Loss: 0.1077, Train: 0.8908 AUC, Valid: 0.7806 AUC, Test: 0.7504 AUC\n",
      "---\n",
      "Run: 01, Epoch: 49, Loss: 0.1083, Train: 0.8998 AUC, Valid: 0.8083 AUC, Test: 0.7628 AUC\n",
      "---\n",
      "Run: 01, Epoch: 50, Loss: 0.1066, Train: 0.9030 AUC, Valid: 0.7990 AUC, Test: 0.7650 AUC\n",
      "---\n",
      "Run: 01, Epoch: 51, Loss: 0.1062, Train: 0.8999 AUC, Valid: 0.8061 AUC, Test: 0.7582 AUC\n",
      "---\n",
      "Run: 01, Epoch: 52, Loss: 0.1047, Train: 0.9042 AUC, Valid: 0.7844 AUC, Test: 0.7537 AUC\n",
      "---\n",
      "Run: 01, Epoch: 53, Loss: 0.1054, Train: 0.9018 AUC, Valid: 0.7826 AUC, Test: 0.7638 AUC\n",
      "---\n",
      "Run: 01, Epoch: 54, Loss: 0.1053, Train: 0.9129 AUC, Valid: 0.7840 AUC, Test: 0.7708 AUC\n",
      "---\n",
      "Run: 01, Epoch: 55, Loss: 0.1033, Train: 0.9117 AUC, Valid: 0.7851 AUC, Test: 0.7547 AUC\n",
      "---\n",
      "Run: 01, Epoch: 56, Loss: 0.1031, Train: 0.9131 AUC, Valid: 0.7917 AUC, Test: 0.7529 AUC\n",
      "---\n",
      "Run: 01, Epoch: 57, Loss: 0.1035, Train: 0.9166 AUC, Valid: 0.7914 AUC, Test: 0.7526 AUC\n",
      "---\n",
      "Run: 01, Epoch: 58, Loss: 0.1015, Train: 0.9212 AUC, Valid: 0.7832 AUC, Test: 0.7605 AUC\n",
      "---\n",
      "Run: 01, Epoch: 59, Loss: 0.1015, Train: 0.9126 AUC, Valid: 0.7835 AUC, Test: 0.7581 AUC\n",
      "---\n",
      "Run: 01, Epoch: 60, Loss: 0.1018, Train: 0.9243 AUC, Valid: 0.7675 AUC, Test: 0.7647 AUC\n",
      "---\n",
      "Run: 01, Epoch: 61, Loss: 0.1009, Train: 0.9200 AUC, Valid: 0.7907 AUC, Test: 0.7425 AUC\n",
      "---\n",
      "Run: 01, Epoch: 62, Loss: 0.1000, Train: 0.9236 AUC, Valid: 0.7768 AUC, Test: 0.7442 AUC\n",
      "---\n",
      "Run: 01, Epoch: 63, Loss: 0.1012, Train: 0.9226 AUC, Valid: 0.7969 AUC, Test: 0.7557 AUC\n",
      "---\n",
      "Run: 01, Epoch: 64, Loss: 0.0994, Train: 0.9292 AUC, Valid: 0.7859 AUC, Test: 0.7588 AUC\n",
      "---\n",
      "Run: 01, Epoch: 65, Loss: 0.0994, Train: 0.9213 AUC, Valid: 0.7921 AUC, Test: 0.7537 AUC\n",
      "---\n",
      "Run: 01, Epoch: 66, Loss: 0.0992, Train: 0.9266 AUC, Valid: 0.7834 AUC, Test: 0.7524 AUC\n",
      "---\n",
      "Run: 01, Epoch: 67, Loss: 0.0971, Train: 0.9351 AUC, Valid: 0.7979 AUC, Test: 0.7506 AUC\n",
      "---\n",
      "Run: 01, Epoch: 68, Loss: 0.0986, Train: 0.9347 AUC, Valid: 0.7902 AUC, Test: 0.7638 AUC\n",
      "---\n",
      "Run: 01, Epoch: 69, Loss: 0.0963, Train: 0.9253 AUC, Valid: 0.7698 AUC, Test: 0.7550 AUC\n",
      "---\n",
      "Run: 01, Epoch: 70, Loss: 0.0959, Train: 0.9351 AUC, Valid: 0.7884 AUC, Test: 0.7414 AUC\n",
      "---\n",
      "Run: 01, Epoch: 71, Loss: 0.0959, Train: 0.9282 AUC, Valid: 0.7752 AUC, Test: 0.7444 AUC\n",
      "---\n",
      "Run: 01, Epoch: 72, Loss: 0.0953, Train: 0.9279 AUC, Valid: 0.7803 AUC, Test: 0.7618 AUC\n",
      "---\n",
      "Run: 01, Epoch: 73, Loss: 0.0952, Train: 0.9370 AUC, Valid: 0.7822 AUC, Test: 0.7545 AUC\n",
      "---\n",
      "Run: 01, Epoch: 74, Loss: 0.0941, Train: 0.9368 AUC, Valid: 0.7721 AUC, Test: 0.7454 AUC\n",
      "---\n",
      "Run: 01, Epoch: 75, Loss: 0.0928, Train: 0.9432 AUC, Valid: 0.7786 AUC, Test: 0.7572 AUC\n",
      "---\n",
      "Run: 01, Epoch: 76, Loss: 0.0931, Train: 0.9392 AUC, Valid: 0.7867 AUC, Test: 0.7395 AUC\n",
      "---\n",
      "Run: 01, Epoch: 77, Loss: 0.0928, Train: 0.9451 AUC, Valid: 0.7794 AUC, Test: 0.7590 AUC\n",
      "---\n",
      "Run: 01, Epoch: 78, Loss: 0.0927, Train: 0.9385 AUC, Valid: 0.7871 AUC, Test: 0.7596 AUC\n",
      "---\n",
      "Run: 01, Epoch: 79, Loss: 0.0914, Train: 0.9481 AUC, Valid: 0.7844 AUC, Test: 0.7592 AUC\n",
      "---\n",
      "Run: 01, Epoch: 80, Loss: 0.0930, Train: 0.9417 AUC, Valid: 0.8077 AUC, Test: 0.7555 AUC\n",
      "---\n",
      "Run: 01, Epoch: 81, Loss: 0.0908, Train: 0.9435 AUC, Valid: 0.7879 AUC, Test: 0.7531 AUC\n",
      "---\n",
      "Run: 01, Epoch: 82, Loss: 0.0906, Train: 0.9472 AUC, Valid: 0.7530 AUC, Test: 0.7496 AUC\n",
      "---\n",
      "Run: 01, Epoch: 83, Loss: 0.0915, Train: 0.9480 AUC, Valid: 0.7792 AUC, Test: 0.7487 AUC\n",
      "---\n",
      "Run: 01, Epoch: 84, Loss: 0.0884, Train: 0.9528 AUC, Valid: 0.7588 AUC, Test: 0.7594 AUC\n",
      "---\n",
      "Run: 01, Epoch: 85, Loss: 0.0905, Train: 0.9511 AUC, Valid: 0.7607 AUC, Test: 0.7582 AUC\n",
      "---\n",
      "Run: 01, Epoch: 86, Loss: 0.0885, Train: 0.9507 AUC, Valid: 0.7652 AUC, Test: 0.7332 AUC\n",
      "---\n",
      "Run: 01, Epoch: 87, Loss: 0.0889, Train: 0.9437 AUC, Valid: 0.7766 AUC, Test: 0.7480 AUC\n",
      "---\n",
      "Run: 01, Epoch: 88, Loss: 0.0895, Train: 0.9527 AUC, Valid: 0.7797 AUC, Test: 0.7457 AUC\n",
      "---\n",
      "Run: 01, Epoch: 89, Loss: 0.0874, Train: 0.9517 AUC, Valid: 0.7701 AUC, Test: 0.7488 AUC\n",
      "---\n",
      "Run: 01, Epoch: 90, Loss: 0.0861, Train: 0.9422 AUC, Valid: 0.7382 AUC, Test: 0.7359 AUC\n",
      "---\n",
      "Run: 01, Epoch: 91, Loss: 0.0868, Train: 0.9485 AUC, Valid: 0.7366 AUC, Test: 0.7509 AUC\n",
      "---\n",
      "Run: 01, Epoch: 92, Loss: 0.0861, Train: 0.9608 AUC, Valid: 0.7588 AUC, Test: 0.7585 AUC\n",
      "---\n",
      "Run: 01, Epoch: 93, Loss: 0.0851, Train: 0.9518 AUC, Valid: 0.7801 AUC, Test: 0.7400 AUC\n",
      "---\n",
      "Run: 01, Epoch: 94, Loss: 0.0859, Train: 0.9604 AUC, Valid: 0.7631 AUC, Test: 0.7524 AUC\n",
      "---\n",
      "Run: 01, Epoch: 95, Loss: 0.0841, Train: 0.9525 AUC, Valid: 0.7546 AUC, Test: 0.7626 AUC\n",
      "---\n",
      "Run: 01, Epoch: 96, Loss: 0.0852, Train: 0.9528 AUC, Valid: 0.7445 AUC, Test: 0.7439 AUC\n",
      "---\n",
      "Run: 01, Epoch: 97, Loss: 0.0856, Train: 0.9533 AUC, Valid: 0.7495 AUC, Test: 0.7405 AUC\n",
      "---\n",
      "Run: 01, Epoch: 98, Loss: 0.0840, Train: 0.9615 AUC, Valid: 0.7698 AUC, Test: 0.7514 AUC\n",
      "---\n",
      "Run: 01, Epoch: 99, Loss: 0.0827, Train: 0.9568 AUC, Valid: 0.7672 AUC, Test: 0.7497 AUC\n",
      "---\n",
      "Run: 01, Epoch: 100, Loss: 0.0847, Train: 0.9591 AUC, Valid: 0.7787 AUC, Test: 0.7413 AUC\n",
      "---\n",
      "Run 01:\n",
      "Highest Train: 96.15\n",
      "Highest Valid: 82.80\n",
      "  Final Train: 85.07\n",
      "   Final Test: 73.13\n",
      "Run: 02, Epoch: 01, Loss: 0.1957, Train: 0.6683 AUC, Valid: 0.6465 AUC, Test: 0.5928 AUC\n",
      "---\n",
      "Run: 02, Epoch: 02, Loss: 0.1515, Train: 0.7138 AUC, Valid: 0.6996 AUC, Test: 0.6190 AUC\n",
      "---\n",
      "Run: 02, Epoch: 03, Loss: 0.1473, Train: 0.7508 AUC, Valid: 0.7796 AUC, Test: 0.6981 AUC\n",
      "---\n",
      "Run: 02, Epoch: 04, Loss: 0.1464, Train: 0.7518 AUC, Valid: 0.7048 AUC, Test: 0.7282 AUC\n",
      "---\n",
      "Run: 02, Epoch: 05, Loss: 0.1435, Train: 0.7608 AUC, Valid: 0.7846 AUC, Test: 0.7440 AUC\n",
      "---\n",
      "Run: 02, Epoch: 06, Loss: 0.1407, Train: 0.7764 AUC, Valid: 0.7667 AUC, Test: 0.7135 AUC\n",
      "---\n",
      "Run: 02, Epoch: 07, Loss: 0.1405, Train: 0.7586 AUC, Valid: 0.6941 AUC, Test: 0.6964 AUC\n",
      "---\n",
      "Run: 02, Epoch: 08, Loss: 0.1398, Train: 0.7791 AUC, Valid: 0.8038 AUC, Test: 0.7348 AUC\n",
      "---\n",
      "Run: 02, Epoch: 09, Loss: 0.1381, Train: 0.7744 AUC, Valid: 0.7398 AUC, Test: 0.7416 AUC\n",
      "---\n",
      "Run: 02, Epoch: 10, Loss: 0.1361, Train: 0.7762 AUC, Valid: 0.7735 AUC, Test: 0.7371 AUC\n",
      "---\n",
      "Run: 02, Epoch: 11, Loss: 0.1335, Train: 0.8039 AUC, Valid: 0.7676 AUC, Test: 0.7363 AUC\n",
      "---\n",
      "Run: 02, Epoch: 12, Loss: 0.1333, Train: 0.7986 AUC, Valid: 0.7573 AUC, Test: 0.7247 AUC\n",
      "---\n",
      "Run: 02, Epoch: 13, Loss: 0.1320, Train: 0.7929 AUC, Valid: 0.7701 AUC, Test: 0.7421 AUC\n",
      "---\n",
      "Run: 02, Epoch: 14, Loss: 0.1311, Train: 0.8083 AUC, Valid: 0.7580 AUC, Test: 0.7102 AUC\n",
      "---\n",
      "Run: 02, Epoch: 15, Loss: 0.1299, Train: 0.8155 AUC, Valid: 0.7807 AUC, Test: 0.7542 AUC\n",
      "---\n",
      "Run: 02, Epoch: 16, Loss: 0.1287, Train: 0.8319 AUC, Valid: 0.7654 AUC, Test: 0.7540 AUC\n",
      "---\n",
      "Run: 02, Epoch: 17, Loss: 0.1268, Train: 0.8281 AUC, Valid: 0.7825 AUC, Test: 0.7487 AUC\n",
      "---\n",
      "Run: 02, Epoch: 18, Loss: 0.1265, Train: 0.8323 AUC, Valid: 0.7574 AUC, Test: 0.7236 AUC\n",
      "---\n",
      "Run: 02, Epoch: 19, Loss: 0.1254, Train: 0.8368 AUC, Valid: 0.7708 AUC, Test: 0.7457 AUC\n",
      "---\n",
      "Run: 02, Epoch: 20, Loss: 0.1247, Train: 0.8390 AUC, Valid: 0.7669 AUC, Test: 0.7470 AUC\n",
      "---\n",
      "Run: 02, Epoch: 21, Loss: 0.1223, Train: 0.8402 AUC, Valid: 0.7789 AUC, Test: 0.7567 AUC\n",
      "---\n",
      "Run: 02, Epoch: 22, Loss: 0.1230, Train: 0.8399 AUC, Valid: 0.7623 AUC, Test: 0.7476 AUC\n",
      "---\n",
      "Run: 02, Epoch: 23, Loss: 0.1225, Train: 0.8544 AUC, Valid: 0.7751 AUC, Test: 0.7546 AUC\n",
      "---\n",
      "Run: 02, Epoch: 24, Loss: 0.1202, Train: 0.8394 AUC, Valid: 0.7538 AUC, Test: 0.7446 AUC\n",
      "---\n",
      "Run: 02, Epoch: 25, Loss: 0.1208, Train: 0.8471 AUC, Valid: 0.7628 AUC, Test: 0.7426 AUC\n",
      "---\n",
      "Run: 02, Epoch: 26, Loss: 0.1208, Train: 0.8572 AUC, Valid: 0.7649 AUC, Test: 0.7540 AUC\n",
      "---\n",
      "Run: 02, Epoch: 27, Loss: 0.1190, Train: 0.8539 AUC, Valid: 0.7709 AUC, Test: 0.7579 AUC\n",
      "---\n",
      "Run: 02, Epoch: 28, Loss: 0.1179, Train: 0.8565 AUC, Valid: 0.7659 AUC, Test: 0.7509 AUC\n",
      "---\n",
      "Run: 02, Epoch: 29, Loss: 0.1182, Train: 0.8578 AUC, Valid: 0.7512 AUC, Test: 0.7560 AUC\n",
      "---\n",
      "Run: 02, Epoch: 30, Loss: 0.1171, Train: 0.8628 AUC, Valid: 0.7631 AUC, Test: 0.7861 AUC\n",
      "---\n",
      "Run: 02, Epoch: 31, Loss: 0.1159, Train: 0.8602 AUC, Valid: 0.7606 AUC, Test: 0.7535 AUC\n",
      "---\n",
      "Run: 02, Epoch: 32, Loss: 0.1163, Train: 0.8694 AUC, Valid: 0.7736 AUC, Test: 0.7561 AUC\n",
      "---\n",
      "Run: 02, Epoch: 33, Loss: 0.1151, Train: 0.8713 AUC, Valid: 0.7808 AUC, Test: 0.7640 AUC\n",
      "---\n",
      "Run: 02, Epoch: 34, Loss: 0.1152, Train: 0.8663 AUC, Valid: 0.7891 AUC, Test: 0.7517 AUC\n",
      "---\n",
      "Run: 02, Epoch: 35, Loss: 0.1139, Train: 0.8690 AUC, Valid: 0.8007 AUC, Test: 0.7477 AUC\n",
      "---\n",
      "Run: 02, Epoch: 36, Loss: 0.1140, Train: 0.8773 AUC, Valid: 0.7903 AUC, Test: 0.7618 AUC\n",
      "---\n",
      "Run: 02, Epoch: 37, Loss: 0.1133, Train: 0.8811 AUC, Valid: 0.7847 AUC, Test: 0.7632 AUC\n",
      "---\n",
      "Run: 02, Epoch: 38, Loss: 0.1111, Train: 0.8684 AUC, Valid: 0.7699 AUC, Test: 0.7317 AUC\n",
      "---\n",
      "Run: 02, Epoch: 39, Loss: 0.1114, Train: 0.8705 AUC, Valid: 0.7574 AUC, Test: 0.7643 AUC\n",
      "---\n",
      "Run: 02, Epoch: 40, Loss: 0.1105, Train: 0.8847 AUC, Valid: 0.7869 AUC, Test: 0.7691 AUC\n",
      "---\n",
      "Run: 02, Epoch: 41, Loss: 0.1111, Train: 0.8870 AUC, Valid: 0.7898 AUC, Test: 0.7350 AUC\n",
      "---\n",
      "Run: 02, Epoch: 42, Loss: 0.1105, Train: 0.8846 AUC, Valid: 0.8006 AUC, Test: 0.7707 AUC\n",
      "---\n",
      "Run: 02, Epoch: 43, Loss: 0.1103, Train: 0.8907 AUC, Valid: 0.7885 AUC, Test: 0.7675 AUC\n",
      "---\n",
      "Run: 02, Epoch: 44, Loss: 0.1112, Train: 0.8953 AUC, Valid: 0.7726 AUC, Test: 0.7666 AUC\n",
      "---\n",
      "Run: 02, Epoch: 45, Loss: 0.1082, Train: 0.8910 AUC, Valid: 0.7754 AUC, Test: 0.7669 AUC\n",
      "---\n",
      "Run: 02, Epoch: 46, Loss: 0.1078, Train: 0.8922 AUC, Valid: 0.7670 AUC, Test: 0.7591 AUC\n",
      "---\n",
      "Run: 02, Epoch: 47, Loss: 0.1077, Train: 0.8906 AUC, Valid: 0.7764 AUC, Test: 0.7593 AUC\n",
      "---\n",
      "Run: 02, Epoch: 48, Loss: 0.1068, Train: 0.9000 AUC, Valid: 0.7869 AUC, Test: 0.7567 AUC\n",
      "---\n",
      "Run: 02, Epoch: 49, Loss: 0.1072, Train: 0.8978 AUC, Valid: 0.7780 AUC, Test: 0.7615 AUC\n",
      "---\n",
      "Run: 02, Epoch: 50, Loss: 0.1071, Train: 0.9016 AUC, Valid: 0.7915 AUC, Test: 0.7775 AUC\n",
      "---\n",
      "Run: 02, Epoch: 51, Loss: 0.1067, Train: 0.9006 AUC, Valid: 0.7772 AUC, Test: 0.7605 AUC\n",
      "---\n",
      "Run: 02, Epoch: 52, Loss: 0.1054, Train: 0.9029 AUC, Valid: 0.7926 AUC, Test: 0.7640 AUC\n",
      "---\n",
      "Run: 02, Epoch: 53, Loss: 0.1050, Train: 0.9033 AUC, Valid: 0.7565 AUC, Test: 0.7521 AUC\n",
      "---\n",
      "Run: 02, Epoch: 54, Loss: 0.1044, Train: 0.8962 AUC, Valid: 0.7762 AUC, Test: 0.7624 AUC\n",
      "---\n",
      "Run: 02, Epoch: 55, Loss: 0.1044, Train: 0.9073 AUC, Valid: 0.7799 AUC, Test: 0.7636 AUC\n",
      "---\n",
      "Run: 02, Epoch: 56, Loss: 0.1033, Train: 0.9146 AUC, Valid: 0.7616 AUC, Test: 0.7677 AUC\n",
      "---\n",
      "Run: 02, Epoch: 57, Loss: 0.1033, Train: 0.9118 AUC, Valid: 0.7750 AUC, Test: 0.7725 AUC\n",
      "---\n",
      "Run: 02, Epoch: 58, Loss: 0.1012, Train: 0.9053 AUC, Valid: 0.7567 AUC, Test: 0.7436 AUC\n",
      "---\n",
      "Run: 02, Epoch: 59, Loss: 0.1037, Train: 0.9146 AUC, Valid: 0.7823 AUC, Test: 0.7664 AUC\n",
      "---\n",
      "Run: 02, Epoch: 60, Loss: 0.1022, Train: 0.9171 AUC, Valid: 0.7638 AUC, Test: 0.7678 AUC\n",
      "---\n",
      "Run: 02, Epoch: 61, Loss: 0.1007, Train: 0.9072 AUC, Valid: 0.7779 AUC, Test: 0.7567 AUC\n",
      "---\n",
      "Run: 02, Epoch: 62, Loss: 0.1008, Train: 0.9146 AUC, Valid: 0.7685 AUC, Test: 0.7790 AUC\n",
      "---\n",
      "Run: 02, Epoch: 63, Loss: 0.0990, Train: 0.9242 AUC, Valid: 0.7525 AUC, Test: 0.7816 AUC\n",
      "---\n",
      "Run: 02, Epoch: 64, Loss: 0.0991, Train: 0.9266 AUC, Valid: 0.7620 AUC, Test: 0.7691 AUC\n",
      "---\n",
      "Run: 02, Epoch: 65, Loss: 0.0989, Train: 0.9256 AUC, Valid: 0.7664 AUC, Test: 0.7537 AUC\n",
      "---\n",
      "Run: 02, Epoch: 66, Loss: 0.0973, Train: 0.9249 AUC, Valid: 0.7685 AUC, Test: 0.7732 AUC\n",
      "---\n",
      "Run: 02, Epoch: 67, Loss: 0.0974, Train: 0.9244 AUC, Valid: 0.7708 AUC, Test: 0.7637 AUC\n",
      "---\n",
      "Run: 02, Epoch: 68, Loss: 0.0979, Train: 0.9243 AUC, Valid: 0.7815 AUC, Test: 0.7854 AUC\n",
      "---\n",
      "Run: 02, Epoch: 69, Loss: 0.0977, Train: 0.9268 AUC, Valid: 0.7767 AUC, Test: 0.7707 AUC\n",
      "---\n",
      "Run: 02, Epoch: 70, Loss: 0.0968, Train: 0.9333 AUC, Valid: 0.7952 AUC, Test: 0.7722 AUC\n",
      "---\n",
      "Run: 02, Epoch: 71, Loss: 0.0961, Train: 0.9290 AUC, Valid: 0.7666 AUC, Test: 0.7728 AUC\n",
      "---\n",
      "Run: 02, Epoch: 72, Loss: 0.0963, Train: 0.9302 AUC, Valid: 0.7575 AUC, Test: 0.7649 AUC\n",
      "---\n",
      "Run: 02, Epoch: 73, Loss: 0.0954, Train: 0.9262 AUC, Valid: 0.7833 AUC, Test: 0.7733 AUC\n",
      "---\n",
      "Run: 02, Epoch: 74, Loss: 0.0947, Train: 0.9361 AUC, Valid: 0.7592 AUC, Test: 0.7649 AUC\n",
      "---\n",
      "Run: 02, Epoch: 75, Loss: 0.0925, Train: 0.9261 AUC, Valid: 0.7549 AUC, Test: 0.7708 AUC\n",
      "---\n",
      "Run: 02, Epoch: 76, Loss: 0.0935, Train: 0.9390 AUC, Valid: 0.7693 AUC, Test: 0.7697 AUC\n",
      "---\n",
      "Run: 02, Epoch: 77, Loss: 0.0932, Train: 0.9321 AUC, Valid: 0.7777 AUC, Test: 0.7708 AUC\n",
      "---\n",
      "Run: 02, Epoch: 78, Loss: 0.0933, Train: 0.9341 AUC, Valid: 0.7495 AUC, Test: 0.7475 AUC\n",
      "---\n",
      "Run: 02, Epoch: 79, Loss: 0.0922, Train: 0.9336 AUC, Valid: 0.7391 AUC, Test: 0.7764 AUC\n",
      "---\n",
      "Run: 02, Epoch: 80, Loss: 0.0927, Train: 0.9355 AUC, Valid: 0.7509 AUC, Test: 0.7573 AUC\n",
      "---\n",
      "Run: 02, Epoch: 81, Loss: 0.0932, Train: 0.9409 AUC, Valid: 0.7569 AUC, Test: 0.7875 AUC\n",
      "---\n",
      "Run: 02, Epoch: 82, Loss: 0.0916, Train: 0.9363 AUC, Valid: 0.7744 AUC, Test: 0.7564 AUC\n",
      "---\n",
      "Run: 02, Epoch: 83, Loss: 0.0899, Train: 0.9352 AUC, Valid: 0.7717 AUC, Test: 0.7728 AUC\n",
      "---\n",
      "Run: 02, Epoch: 84, Loss: 0.0907, Train: 0.9433 AUC, Valid: 0.7568 AUC, Test: 0.7513 AUC\n",
      "---\n",
      "Run: 02, Epoch: 85, Loss: 0.0908, Train: 0.9417 AUC, Valid: 0.7518 AUC, Test: 0.7773 AUC\n",
      "---\n",
      "Run: 02, Epoch: 86, Loss: 0.0908, Train: 0.9404 AUC, Valid: 0.7658 AUC, Test: 0.7577 AUC\n",
      "---\n",
      "Run: 02, Epoch: 87, Loss: 0.0894, Train: 0.9452 AUC, Valid: 0.7644 AUC, Test: 0.7590 AUC\n",
      "---\n",
      "Run: 02, Epoch: 88, Loss: 0.0897, Train: 0.9491 AUC, Valid: 0.7729 AUC, Test: 0.7579 AUC\n",
      "---\n",
      "Run: 02, Epoch: 89, Loss: 0.0888, Train: 0.9428 AUC, Valid: 0.7586 AUC, Test: 0.7594 AUC\n",
      "---\n",
      "Run: 02, Epoch: 90, Loss: 0.0890, Train: 0.9487 AUC, Valid: 0.7767 AUC, Test: 0.7420 AUC\n",
      "---\n",
      "Run: 02, Epoch: 91, Loss: 0.0870, Train: 0.9471 AUC, Valid: 0.7557 AUC, Test: 0.7586 AUC\n",
      "---\n",
      "Run: 02, Epoch: 92, Loss: 0.0868, Train: 0.9480 AUC, Valid: 0.7683 AUC, Test: 0.7586 AUC\n",
      "---\n",
      "Run: 02, Epoch: 93, Loss: 0.0867, Train: 0.9494 AUC, Valid: 0.7831 AUC, Test: 0.7555 AUC\n",
      "---\n",
      "Run: 02, Epoch: 94, Loss: 0.0868, Train: 0.9493 AUC, Valid: 0.7614 AUC, Test: 0.7596 AUC\n",
      "---\n",
      "Run: 02, Epoch: 95, Loss: 0.0863, Train: 0.9560 AUC, Valid: 0.7680 AUC, Test: 0.7491 AUC\n",
      "---\n",
      "Run: 02, Epoch: 96, Loss: 0.0851, Train: 0.9549 AUC, Valid: 0.7681 AUC, Test: 0.7579 AUC\n",
      "---\n",
      "Run: 02, Epoch: 97, Loss: 0.0862, Train: 0.9534 AUC, Valid: 0.7664 AUC, Test: 0.7386 AUC\n",
      "---\n",
      "Run: 02, Epoch: 98, Loss: 0.0842, Train: 0.9565 AUC, Valid: 0.7681 AUC, Test: 0.7566 AUC\n",
      "---\n",
      "Run: 02, Epoch: 99, Loss: 0.0856, Train: 0.9521 AUC, Valid: 0.7843 AUC, Test: 0.7625 AUC\n",
      "---\n",
      "Run: 02, Epoch: 100, Loss: 0.0837, Train: 0.9558 AUC, Valid: 0.7776 AUC, Test: 0.7571 AUC\n",
      "---\n",
      "Run 02:\n",
      "Highest Train: 95.65\n",
      "Highest Valid: 80.38\n",
      "  Final Train: 77.91\n",
      "   Final Test: 73.48\n",
      "Run: 03, Epoch: 01, Loss: 0.1926, Train: 0.6406 AUC, Valid: 0.6336 AUC, Test: 0.5211 AUC\n",
      "---\n",
      "Run: 03, Epoch: 02, Loss: 0.1503, Train: 0.7169 AUC, Valid: 0.6843 AUC, Test: 0.6623 AUC\n",
      "---\n",
      "Run: 03, Epoch: 03, Loss: 0.1470, Train: 0.7449 AUC, Valid: 0.7472 AUC, Test: 0.7321 AUC\n",
      "---\n",
      "Run: 03, Epoch: 04, Loss: 0.1442, Train: 0.7612 AUC, Valid: 0.7694 AUC, Test: 0.7248 AUC\n",
      "---\n",
      "Run: 03, Epoch: 05, Loss: 0.1433, Train: 0.7543 AUC, Valid: 0.7434 AUC, Test: 0.6989 AUC\n",
      "---\n",
      "Run: 03, Epoch: 06, Loss: 0.1416, Train: 0.7776 AUC, Valid: 0.7722 AUC, Test: 0.7307 AUC\n",
      "---\n",
      "Run: 03, Epoch: 07, Loss: 0.1403, Train: 0.7555 AUC, Valid: 0.7546 AUC, Test: 0.7104 AUC\n",
      "---\n",
      "Run: 03, Epoch: 08, Loss: 0.1384, Train: 0.7861 AUC, Valid: 0.7573 AUC, Test: 0.7437 AUC\n",
      "---\n",
      "Run: 03, Epoch: 09, Loss: 0.1362, Train: 0.7871 AUC, Valid: 0.7631 AUC, Test: 0.6898 AUC\n",
      "---\n",
      "Run: 03, Epoch: 10, Loss: 0.1352, Train: 0.8063 AUC, Valid: 0.7737 AUC, Test: 0.7587 AUC\n",
      "---\n",
      "Run: 03, Epoch: 11, Loss: 0.1337, Train: 0.7929 AUC, Valid: 0.7708 AUC, Test: 0.7067 AUC\n",
      "---\n",
      "Run: 03, Epoch: 12, Loss: 0.1336, Train: 0.8077 AUC, Valid: 0.7650 AUC, Test: 0.7278 AUC\n",
      "---\n",
      "Run: 03, Epoch: 13, Loss: 0.1314, Train: 0.7940 AUC, Valid: 0.7408 AUC, Test: 0.7239 AUC\n",
      "---\n",
      "Run: 03, Epoch: 14, Loss: 0.1302, Train: 0.8040 AUC, Valid: 0.7547 AUC, Test: 0.7287 AUC\n",
      "---\n",
      "Run: 03, Epoch: 15, Loss: 0.1301, Train: 0.8188 AUC, Valid: 0.7830 AUC, Test: 0.7499 AUC\n",
      "---\n",
      "Run: 03, Epoch: 16, Loss: 0.1291, Train: 0.8146 AUC, Valid: 0.7685 AUC, Test: 0.7307 AUC\n",
      "---\n",
      "Run: 03, Epoch: 17, Loss: 0.1291, Train: 0.8226 AUC, Valid: 0.7802 AUC, Test: 0.7512 AUC\n",
      "---\n",
      "Run: 03, Epoch: 18, Loss: 0.1275, Train: 0.8301 AUC, Valid: 0.7940 AUC, Test: 0.7446 AUC\n",
      "---\n",
      "Run: 03, Epoch: 19, Loss: 0.1269, Train: 0.8296 AUC, Valid: 0.7909 AUC, Test: 0.7430 AUC\n",
      "---\n",
      "Run: 03, Epoch: 20, Loss: 0.1268, Train: 0.8322 AUC, Valid: 0.7879 AUC, Test: 0.7351 AUC\n",
      "---\n",
      "Run: 03, Epoch: 21, Loss: 0.1254, Train: 0.8315 AUC, Valid: 0.7650 AUC, Test: 0.7304 AUC\n",
      "---\n",
      "Run: 03, Epoch: 22, Loss: 0.1241, Train: 0.8356 AUC, Valid: 0.7585 AUC, Test: 0.7062 AUC\n",
      "---\n",
      "Run: 03, Epoch: 23, Loss: 0.1230, Train: 0.8439 AUC, Valid: 0.7707 AUC, Test: 0.7428 AUC\n",
      "---\n",
      "Run: 03, Epoch: 24, Loss: 0.1211, Train: 0.8393 AUC, Valid: 0.7695 AUC, Test: 0.7342 AUC\n",
      "---\n",
      "Run: 03, Epoch: 25, Loss: 0.1224, Train: 0.8468 AUC, Valid: 0.8027 AUC, Test: 0.7400 AUC\n",
      "---\n",
      "Run: 03, Epoch: 26, Loss: 0.1209, Train: 0.8421 AUC, Valid: 0.7848 AUC, Test: 0.7662 AUC\n",
      "---\n",
      "Run: 03, Epoch: 27, Loss: 0.1199, Train: 0.8509 AUC, Valid: 0.7696 AUC, Test: 0.7406 AUC\n",
      "---\n",
      "Run: 03, Epoch: 28, Loss: 0.1198, Train: 0.8493 AUC, Valid: 0.7710 AUC, Test: 0.7383 AUC\n",
      "---\n",
      "Run: 03, Epoch: 29, Loss: 0.1185, Train: 0.8538 AUC, Valid: 0.7820 AUC, Test: 0.7580 AUC\n",
      "---\n",
      "Run: 03, Epoch: 30, Loss: 0.1190, Train: 0.8534 AUC, Valid: 0.7803 AUC, Test: 0.7499 AUC\n",
      "---\n",
      "Run: 03, Epoch: 31, Loss: 0.1181, Train: 0.8524 AUC, Valid: 0.7545 AUC, Test: 0.7362 AUC\n",
      "---\n",
      "Run: 03, Epoch: 32, Loss: 0.1177, Train: 0.8621 AUC, Valid: 0.7773 AUC, Test: 0.7549 AUC\n",
      "---\n",
      "Run: 03, Epoch: 33, Loss: 0.1167, Train: 0.8720 AUC, Valid: 0.7896 AUC, Test: 0.7532 AUC\n",
      "---\n",
      "Run: 03, Epoch: 34, Loss: 0.1168, Train: 0.8580 AUC, Valid: 0.7893 AUC, Test: 0.7514 AUC\n",
      "---\n",
      "Run: 03, Epoch: 35, Loss: 0.1152, Train: 0.8706 AUC, Valid: 0.7797 AUC, Test: 0.7213 AUC\n",
      "---\n",
      "Run: 03, Epoch: 36, Loss: 0.1148, Train: 0.8611 AUC, Valid: 0.8170 AUC, Test: 0.7510 AUC\n",
      "---\n",
      "Run: 03, Epoch: 37, Loss: 0.1145, Train: 0.8706 AUC, Valid: 0.7913 AUC, Test: 0.7339 AUC\n",
      "---\n",
      "Run: 03, Epoch: 38, Loss: 0.1138, Train: 0.8806 AUC, Valid: 0.8014 AUC, Test: 0.7569 AUC\n",
      "---\n",
      "Run: 03, Epoch: 39, Loss: 0.1125, Train: 0.8774 AUC, Valid: 0.7876 AUC, Test: 0.7306 AUC\n",
      "---\n",
      "Run: 03, Epoch: 40, Loss: 0.1132, Train: 0.8809 AUC, Valid: 0.7875 AUC, Test: 0.7583 AUC\n",
      "---\n",
      "Run: 03, Epoch: 41, Loss: 0.1121, Train: 0.8641 AUC, Valid: 0.7644 AUC, Test: 0.7158 AUC\n",
      "---\n",
      "Run: 03, Epoch: 42, Loss: 0.1114, Train: 0.8850 AUC, Valid: 0.7802 AUC, Test: 0.7564 AUC\n",
      "---\n",
      "Run: 03, Epoch: 43, Loss: 0.1091, Train: 0.8791 AUC, Valid: 0.7681 AUC, Test: 0.7457 AUC\n",
      "---\n",
      "Run: 03, Epoch: 44, Loss: 0.1101, Train: 0.8810 AUC, Valid: 0.7916 AUC, Test: 0.7348 AUC\n",
      "---\n",
      "Run: 03, Epoch: 45, Loss: 0.1099, Train: 0.8926 AUC, Valid: 0.7651 AUC, Test: 0.7566 AUC\n",
      "---\n",
      "Run: 03, Epoch: 46, Loss: 0.1078, Train: 0.8905 AUC, Valid: 0.7985 AUC, Test: 0.7539 AUC\n",
      "---\n",
      "Run: 03, Epoch: 47, Loss: 0.1073, Train: 0.9018 AUC, Valid: 0.7947 AUC, Test: 0.7670 AUC\n",
      "---\n",
      "Run: 03, Epoch: 48, Loss: 0.1075, Train: 0.8919 AUC, Valid: 0.8027 AUC, Test: 0.7504 AUC\n",
      "---\n",
      "Run: 03, Epoch: 49, Loss: 0.1077, Train: 0.8891 AUC, Valid: 0.8080 AUC, Test: 0.7381 AUC\n",
      "---\n",
      "Run: 03, Epoch: 50, Loss: 0.1077, Train: 0.8969 AUC, Valid: 0.7905 AUC, Test: 0.7623 AUC\n",
      "---\n",
      "Run: 03, Epoch: 51, Loss: 0.1058, Train: 0.8963 AUC, Valid: 0.7902 AUC, Test: 0.7429 AUC\n",
      "---\n",
      "Run: 03, Epoch: 52, Loss: 0.1050, Train: 0.8993 AUC, Valid: 0.7903 AUC, Test: 0.7413 AUC\n",
      "---\n",
      "Run: 03, Epoch: 53, Loss: 0.1048, Train: 0.9010 AUC, Valid: 0.7758 AUC, Test: 0.7577 AUC\n",
      "---\n",
      "Run: 03, Epoch: 54, Loss: 0.1048, Train: 0.8985 AUC, Valid: 0.7786 AUC, Test: 0.7491 AUC\n",
      "---\n",
      "Run: 03, Epoch: 55, Loss: 0.1037, Train: 0.9045 AUC, Valid: 0.7939 AUC, Test: 0.7536 AUC\n",
      "---\n",
      "Run: 03, Epoch: 56, Loss: 0.1030, Train: 0.9067 AUC, Valid: 0.7999 AUC, Test: 0.7587 AUC\n",
      "---\n",
      "Run: 03, Epoch: 57, Loss: 0.1033, Train: 0.9062 AUC, Valid: 0.7848 AUC, Test: 0.7558 AUC\n",
      "---\n",
      "Run: 03, Epoch: 58, Loss: 0.1025, Train: 0.9002 AUC, Valid: 0.8023 AUC, Test: 0.7479 AUC\n",
      "---\n",
      "Run: 03, Epoch: 59, Loss: 0.1036, Train: 0.9112 AUC, Valid: 0.7692 AUC, Test: 0.7624 AUC\n",
      "---\n",
      "Run: 03, Epoch: 60, Loss: 0.1015, Train: 0.8966 AUC, Valid: 0.7950 AUC, Test: 0.7641 AUC\n",
      "---\n",
      "Run: 03, Epoch: 61, Loss: 0.1014, Train: 0.9142 AUC, Valid: 0.7897 AUC, Test: 0.7568 AUC\n",
      "---\n",
      "Run: 03, Epoch: 62, Loss: 0.1012, Train: 0.9094 AUC, Valid: 0.7833 AUC, Test: 0.7669 AUC\n",
      "---\n",
      "Run: 03, Epoch: 63, Loss: 0.1008, Train: 0.9167 AUC, Valid: 0.7988 AUC, Test: 0.7588 AUC\n",
      "---\n",
      "Run: 03, Epoch: 64, Loss: 0.0986, Train: 0.9104 AUC, Valid: 0.7831 AUC, Test: 0.7620 AUC\n",
      "---\n",
      "Run: 03, Epoch: 65, Loss: 0.0995, Train: 0.9081 AUC, Valid: 0.7954 AUC, Test: 0.7503 AUC\n",
      "---\n",
      "Run: 03, Epoch: 66, Loss: 0.0998, Train: 0.9211 AUC, Valid: 0.7895 AUC, Test: 0.7568 AUC\n",
      "---\n",
      "Run: 03, Epoch: 67, Loss: 0.0987, Train: 0.9212 AUC, Valid: 0.7954 AUC, Test: 0.7613 AUC\n",
      "---\n",
      "Run: 03, Epoch: 68, Loss: 0.0980, Train: 0.9179 AUC, Valid: 0.7992 AUC, Test: 0.7795 AUC\n",
      "---\n",
      "Run: 03, Epoch: 69, Loss: 0.0975, Train: 0.9197 AUC, Valid: 0.7837 AUC, Test: 0.7754 AUC\n",
      "---\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m logger \u001b[39m=\u001b[39m repeat_experiments(\n\u001b[1;32m      2\u001b[0m     model, train_dataloader, val_dataloader, test_dataloader, \n\u001b[1;32m      3\u001b[0m     device, train_args, N_runs)\n",
      "File \u001b[0;32m~/repos/dgl2/utils.py:180\u001b[0m, in \u001b[0;36mrepeat_experiments\u001b[0;34m(model, dl_train, dl_val, dl_test, device, train_args, n_runs)\u001b[0m\n\u001b[1;32m    167\u001b[0m     logger \u001b[39m=\u001b[39m train(\n\u001b[1;32m    168\u001b[0m         model,\n\u001b[1;32m    169\u001b[0m         dl_train,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    175\u001b[0m         run,\n\u001b[1;32m    176\u001b[0m     )\n\u001b[1;32m    178\u001b[0m     logger\u001b[39m.\u001b[39mprint_statistics(run)\n\u001b[0;32m--> 180\u001b[0m logger\u001b[39m.\u001b[39mprint_statistics()\n\u001b[1;32m    181\u001b[0m \u001b[39mreturn\u001b[39;00m logger\n",
      "File \u001b[0;32m~/repos/dgl2/utils.py:152\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dl_train, dl_val, dl_test, device, logger, args, run)\u001b[0m\n\u001b[1;32m    143\u001b[0m         logger\u001b[39m.\u001b[39madd_result(run, result)\n\u001b[1;32m    145\u001b[0m         \u001b[39mif\u001b[39;00m epoch \u001b[39m%\u001b[39m args[\u001b[39m\"\u001b[39m\u001b[39mlog_steps\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    146\u001b[0m             \u001b[39mprint\u001b[39m(\n\u001b[1;32m    147\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mRun: \u001b[39m\u001b[39m{\u001b[39;00mrun\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m:\u001b[39;00m\u001b[39m02d\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    148\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m:\u001b[39;00m\u001b[39m02d\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    149\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLoss: \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    150\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTrain: \u001b[39m\u001b[39m{\u001b[39;00mtrain_roc\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m AUC, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    151\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mValid: \u001b[39m\u001b[39m{\u001b[39;00mval_roc\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m AUC, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 152\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTest: \u001b[39m\u001b[39m{\u001b[39;00mtest_roc\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m AUC\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    153\u001b[0m             )\n\u001b[1;32m    154\u001b[0m             \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m---\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    156\u001b[0m \u001b[39mreturn\u001b[39;00m logger\n",
      "File \u001b[0;32m~/repos/dgl2/venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/repos/dgl2/utils.py:123\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(model, loader, device)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain\u001b[39m(model, dl_train, dl_val, dl_test, device, logger, args, run):\n\u001b[1;32m    122\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m \u001b[39m    A complete model training loop\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m    125\u001b[0m     optimizer \u001b[39m=\u001b[39m th\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(\n\u001b[1;32m    126\u001b[0m         model\u001b[39m.\u001b[39mparameters(),\n\u001b[1;32m    127\u001b[0m         lr\u001b[39m=\u001b[39margs[\u001b[39m\"\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m    128\u001b[0m     )\n\u001b[1;32m    130\u001b[0m     \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m \u001b[39m+\u001b[39m args[\u001b[39m\"\u001b[39m\u001b[39mepochs\u001b[39m\u001b[39m\"\u001b[39m]):\n",
      "File \u001b[0;32m~/repos/dgl2/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[29], line 18\u001b[0m, in \u001b[0;36mGraphGNN.forward\u001b[0;34m(self, g)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, g): \u001b[39m# !!! g is a batched super-graph\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m     h_node \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnode_GNN(g) \u001b[39m# node-level embeddings with self.node_GNN\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     \u001b[39m# pool the node-level embedding to get graph representations\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     \u001b[39mwith\u001b[39;00m g\u001b[39m.\u001b[39mlocal_scope():\n",
      "File \u001b[0;32m~/repos/dgl2/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[24], line 42\u001b[0m, in \u001b[0;36mNodeGNN.forward\u001b[0;34m(self, g)\u001b[0m\n\u001b[1;32m     39\u001b[0m edge_embedding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbond_encoder(g\u001b[39m.\u001b[39medata[\u001b[39m'\u001b[39m\u001b[39mfeat\u001b[39m\u001b[39m'\u001b[39m]) \u001b[39m# edge embeddings\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers):\n\u001b[0;32m---> 42\u001b[0m     h \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvs[layer](g, h, edge_embedding)\n\u001b[1;32m     43\u001b[0m     h \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_norms[layer](h)\n\u001b[1;32m     44\u001b[0m     \u001b[39mif\u001b[39;00m layer \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m     45\u001b[0m         \u001b[39m#remove relu for the last layer\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/dgl2/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[20], line 60\u001b[0m, in \u001b[0;36mGINLayer.forward\u001b[0;34m(self, g, x_node, x_edge)\u001b[0m\n\u001b[1;32m     57\u001b[0m     h_mp \u001b[39m=\u001b[39m g\u001b[39m.\u001b[39mdstdata[\u001b[39m'\u001b[39m\u001b[39mmp\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m# will be used in the next layer\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[39m# GIN update equation\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmlp((\u001b[39m1\u001b[39;49m \u001b[39m+\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps) \u001b[39m*\u001b[39;49m x_node \u001b[39m+\u001b[39;49m h_mp)\n\u001b[1;32m     62\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/repos/dgl2/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1494\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1491\u001b[0m             tracing_state\u001b[39m.\u001b[39mpop_scope()\n\u001b[1;32m   1492\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n\u001b[0;32m-> 1494\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call_impl\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m   1495\u001b[0m     forward_call \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_get_tracing_state() \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward)\n\u001b[1;32m   1496\u001b[0m     \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m     \u001b[39m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "logger = repeat_experiments(\n",
    "    model, train_dataloader, val_dataloader, test_dataloader, \n",
    "    device, train_args, N_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'logger' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Final performance\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m logger\u001b[39m.\u001b[39mprint_statistics()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'logger' is not defined"
     ]
    }
   ],
   "source": [
    "# Final performance\n",
    "logger.print_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAikAAAGzCAYAAADqhoemAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAB/7klEQVR4nO3dd3hTZfvA8W+StumedFAoUMree8sQREAQFAUXioroD/RVEfXFLai4RWXpq+JCGSqgqIBsVDaUvWlZpYUWunfz/P44TaDQQkdW2/tzXbk4PTnnOXcOaXL3mTqllEIIIYQQwsnoHR2AEEIIIURxJEkRQgghhFOSJEUIIYQQTkmSFCGEEEI4JUlShBBCCOGUJEkRQgghhFOSJEUIIYQQTkmSFCGEEEI4JUlShBBCCOGUJEkRQlR6sbGx6HQ6vv76a0eHIoSwIklSqjidTleqx9q1ayt8rczMTF577bVSl7V27doiMRgMBkJCQrjjjjs4cOBAiectXbqUAQMGEBQUhLu7O40aNWLixIkkJSVd81q33347YWFhuLm5ERISwpAhQ/jll19K/foKCgoIDw9Hp9Px559/FnvM6NGj8fb2LrEMb29vRo8efdX+hIQEJk6cSJMmTfD09MTLy4v27dvzxhtvkJycXKr4oqOjue+++4iIiMBoNBIYGEi/fv2YM2cOBQUFpSpDVG9nz57lv//9L3369MHHx+e6nw3//vsvPXr0wNPTk7CwMP7zn/+Qnp5uv4BFlefi6ACEbX333XdFfv7222/566+/rtrftGnTCl8rMzOT119/HYDevXuX+rz//Oc/dOzYkby8PHbv3s3s2bNZu3Yte/fuJSwsrMixEydO5IMPPqB169Y8//zzBAYGsmPHDqZPn868efNYtWoVjRs3LnLOq6++yuTJk2nYsCGPPvoodevWJSkpiT/++IPhw4czd+5c7rnnnuvGuXr1as6ePUu9evWYO3cuAwcOLPVrvJatW7cyaNAg0tPTue+++2jfvj0A27Zt4+2332b9+vWsWLHimmV88cUXPPbYY4SGhjJq1CgaNmxIWloaq1at4uGHH+bs2bO88MILVonXGdWtW5esrCxcXV0dHUqldujQId555x0aNmxIy5Yt2bhxY4nHRkdH07dvX5o2bcqHH37I6dOnef/99zly5EiJSbwQZaZEtTJ+/Hhlq//28+fPK0C9+uqrpTp+zZo1ClALFy4ssn/WrFkKUO+8806R/T/88IMC1MiRI1V+fn6R5zZv3qw8PT1Vy5YtVV5enmX/woULFaDuuOMOlZube1UMy5YtU7/99lup4r3//vtVu3bt1Mcff6y8vLxUenr6Vcc88MADysvLq8QyvLy81AMPPGD5+eLFi6pWrVoqNDRUHThw4Krj4+Pj1ZQpU64Z18aNG5XBYFA9evRQqampVz2/detWNWfOnGuWUVnl5eWpnJwcR4dRqWRkZJT4XGpqqkpKSlJKXfrdWbNmTbHHDhw4UNWsWVOlpKRY9v3vf/9TgFq+fHmZ48rKylIFBQVlPk9UbZKkVDPFJSkFBQXqo48+Us2aNVNGo1GFhISosWPHqgsXLhQ5buvWrap///4qKChIubu7q3r16qkHH3xQKaVUTEyMAq56XCthKSlJ2bt3rwLU2LFji+xv3LixCggIKPKheLnXX39dAerHH3+07GvSpIkKDAws9su7LDIzM5WPj49699131dmzZ5Ver1dz58696riyJilvv/22Aootq7QGDBigXFxc1IkTJ0p1fHp6upowYYKqXbu2cnNzU40aNVLvvfeeMplMRY4D1Pjx49WCBQtU06ZNlbu7u+rSpYvavXu3Ukqp2bNnq6ioKGU0GlWvXr1UTExMkfN79eqlmjdvrrZt26a6du1qec/MmjWryHE5OTnq5ZdfVu3atVO+vr7K09NT9ejRQ61evbrIceb32Hvvvac++ugjVb9+faXX69XOnTstz12ejJ09e1aNHj1a1apVS7m5uamwsDB16623XhXnjBkzVLNmzZSbm5uqWbOmGjdunLp48WKxr2Xfvn2qd+/eysPDQ4WHh1+VSJckLy9PTZ48WdWvX1+5ubmpunXrqkmTJqns7GzLMbfccouKjIws9vwuXbqo9u3bF9n33XffqXbt2il3d3cVEBCgRo4cqU6ePFls3Nu2bVM33HCD8vDwUE8++WSpYr5WkpKSkqJcXFzUs88+W2R/Tk6O8vb2Vg8//PA1yzb/7v/444/qxRdfVOHh4Uqn06mLFy+qV199tdg/pObMmaOAIv9/devWVbfccovasGGD6tixozIajSoyMlJ98803Rc7Nzc1Vr732mmrQoIEyGo0qMDBQde/eXa1YsaJU90I4jiQp1UxxScqYMWOUi4uLeuSRR9Ts2bPV888/r7y8vFTHjh0ttQ8JCQkqICDA8oX2v//9T7344ouqadOmSinti89cA3Lbbbep7777Tn333Xdq165dJcZSUpKydOlSBajnn3/esu/w4cMKUKNHjy6xPPMX1b333lvknIceeqhsN6kY8+bNUzqdzvIlcOONN6pBgwZddVxZk5Ru3bopDw+PctcGZGRkKFdXV3XjjTeW6niTyaRuvPFGpdPp1JgxY9T06dPVkCFDFKCeeuqpIscCqlWrVioiIkK9/fbb6u2331Z+fn6qTp06avr06apZs2bqgw8+UC+99JJyc3NTffr0KXJ+r169VHh4uAoJCVGPP/64+uSTT1SPHj0UoL788kvLcefPn1c1a9ZUEyZMULNmzVLvvvuuaty4sXJ1dVU7d+60HGf+/23WrJmqX7++evvtt9VHH32kTpw4UWyS0q1bN+Xn56deeukl9cUXX6i33npL9enTR61bt85yjPkLsV+/furTTz9Vjz/+uDIYDEXe+5e/loiICPXkk0+qmTNnqhtvvFEB6o8//rjufX/ggQcsNXozZsxQ999/vwLUsGHDLMd8++23ClBbtmwpcm5sbKwlOTN74403lE6nUyNHjlQzZ85Ur7/+uqpRo4aqV69ekQSrV69eKiwsTAUHB6snnnhCffbZZ2rx4sXXjVepaycpf//9twLU/Pnzr3quR48eql27dtcs2/y736xZM9WmTRv14YcfqqlTp6qMjIwyJymNGzdWoaGh6oUXXlDTp09X7dq1UzqdTu3du9dy3AsvvKB0Op165JFH1P/+9z/1wQcfqLvvvlu9/fbbpboXwnEkSalmrkxSNmzYUOxf8suWLSuyf9GiRQpQW7duLbHs8jb3fPXVV+r8+fMqLi5OLVu2TDVo0EDpdLoiH9aLFy9WgProo4+uWaavr6/lA3LJkiWlOqc0Bg8erLp37275+fPPP1cuLi7q3LlzRY4ra5ISEBCgWrduXe64du3apYBS/3Vsvo9vvPFGkf133HGH0ul06ujRo5Z9gDIajUW+FD777DMFqLCwsCK1U5MmTbrqC6RXr14KUB988IFlX05OjmrTpo0KCQmxJAH5+flXJWkXL15UoaGhRRJMcyLi6+t71X2/Mkm5ePHiVV/sVzp37pxyc3NT/fv3L9LMMH36dMv78srX8u233xZ5LWFhYWr48OElXkMppaKjoxWgxowZU2T/xIkTFWCpMUpJSVFGo1E988wzRY579913lU6ns9SUxcbGKoPBoN58880ix+3Zs0e5uLgU2W+Oe/bs2deMsTjXSlLMz61fv/6q5+68804VFhZ2zbLNv/v169dXmZmZRZ4ra5JyZRznzp276j62bt1a3XLLLdeMSTgnGd1TzS1cuBA/Pz9uuukmEhMTLY/27dvj7e3NmjVrAPD39we0kTV5eXlWjeGhhx4iODiY8PBwBgwYQEpKCt999x0dO3a0HJOWlgaAj4/PNcvy8fEhNTUVwPLv9c65nqSkJJYvX87dd99t2Td8+HB0Oh0LFiyoUNmpqakViq+sr/GPP/7AYDDwn//8p8j+Z555BqXUVR0e+/btS7169Sw/d+7cGdBe/+XXNO8/fvx4kfNdXFx49NFHLT+7ubnx6KOPcu7cObZv3w6AwWDAzc0NAJPJxIULF8jPz6dDhw7s2LHjqtcwfPhwgoODr/k6PTw8cHNzY+3atVy8eLHYY1auXElubi5PPfUUev2lj8JHHnkEX19ffv/99yLHe3t7c9999xV5LZ06dbrqNV/pjz/+AGDChAlF9j/zzDMAluv4+voycOBAFixYgFLKctz8+fPp0qULderUAeCXX37BZDIxYsSIIr+zYWFhNGzY0PI7a2Y0GnnwwQevGWNZZWVlWcq+kru7u+X563nggQfw8PCoUCzNmjXjhhtusPwcHBxM48aNi/y/+Pv7s2/fPo4cOVKhawn7kySlmjty5AgpKSmEhIQQHBxc5JGens65c+cA6NWrF8OHD+f111+nRo0aDB06lDlz5pCTk1PhGF555RX++usvFi1axP33309KSkqRLw249CVsTlZKkpaWZjnW19e3VOdcz/z588nLy6Nt27YcPXqUo0ePcuHCBTp37szcuXPLXJ5Op7Ns+/r6Vii+sr7GEydOEB4eflVSYx7ddeLEiSL7zV+MZn5+fgBEREQUu//KhCA8PBwvL68i+xo1agRoc5uYffPNN7Rq1Qp3d3eCgoIIDg7m999/JyUl5arXEBkZec3XCNqX5zvvvMOff/5JaGgoPXv25N133yU+Pt5yjPm1XjkazM3Njfr16191L2rXrl3k/w4gICCgxCTo8uvo9XoaNGhQZH9YWBj+/v5FrjNy5EhOnTplGVVz7Ngxtm/fzsiRIy3HHDlyBKUUDRs2vOp39sCBA5bfWbNatWpZkkBrMScWxf3+Z2dnlzrxKM3/5fVc+R6Fq/9fJk+eTHJyMo0aNaJly5Y8++yz7N69u8LXFrYnQ5CrOZPJREhISIlftua/WHU6HT/99BObNm3it99+Y/ny5Tz00EN88MEHbNq06Zpzg1xPy5Yt6devHwDDhg0jMzOTRx55hB49eli+DM1fotf6YDlx4gSpqak0a9YMgCZNmgCwZ8+ecscGWO5N9+7di33++PHj1K9fH9D+iszJyUEpddUXmlKK7Oxs3N3dLfuaNGlCdHQ0ubm55foiadCgAS4uLhV+jSUxGAxl2n95DUBpff/994wePZphw4bx7LPPEhISgsFgYOrUqRw7duyq40v7BfjUU08xZMgQFi9ezPLly3n55ZeZOnUqq1evpm3btmWOs6Kv+cr3Q3GGDBmCp6cnCxYsoFu3bixYsAC9Xs+dd95pOcZkMlnm6ikupit/FytaU1GcmjVrAtq8Klc6e/Ys4eHhpSqnuNhKuk8lzfVTmv+Xnj17cuzYMZYsWcKKFSv44osv+Oijj5g9ezZjxowpVazCMaQmpZqLiooiKSmJ7t27069fv6serVu3LnJ8ly5dePPNN9m2bRtz585l3759zJs3Dyjdh3BpvP3222RnZ/Pmm29a9jVq1IhGjRqxePHiEmsNvv32WwAGDx5sOadx48YsWbKk3BNMxcTE8O+///L444+zcOHCIo/58+fj5ubGDz/8YDm+bt265OfnF/vlevToUQoKCqhbt65l35AhQ8jKyuLnn38uV3yenp7ceOONrF+/nlOnTl33+Lp16xIXF3fVPTx48KDleWuKi4sjIyOjyL7Dhw8DWJqRfvrpJ+rXr88vv/zCqFGjuPnmm+nXrx/Z2dkVvn5UVBTPPPMMK1asYO/eveTm5vLBBx8Al17roUOHipyTm5tLTEyM1e5F3bp1MZlMVzU1JCQkkJycXOQ6Xl5eDB48mIULF2IymZg/fz433HBDkS/9qKgolFJERkYW+zvbpUsXq8R9LS1atMDFxYVt27YV2Z+bm0t0dDRt2rQpd9kBAQEAV01ieGXNVlkFBgby4IMP8uOPP3Lq1ClatWrFa6+9VqEyhe1JklLNjRgxgoKCAqZMmXLVc/n5+ZYPiosXL171F6P5g8hc5evp6Qlc/eFSVlFRUQwfPpyvv/66SPX8K6+8wsWLF3nssceu+qtq+/btvPPOO7Ro0YLhw4db9r/++uskJSUxZswY8vPzr7rWihUrWLp0aYmxmGtRnnvuOe64444ijxEjRtCrV68itVDmCd6mT59+VVkzZswocgzAY489Rs2aNXnmmWcsX96XO3fuHG+88UaJ8YE2WZ1SilGjRhWbjG3fvp1vvvkGgEGDBlFQUHBVfB999BE6nc5qE9SZ5efn89lnn1l+zs3N5bPPPiM4ONgyaZ35L+HL31+bN2++5kRi15OZmXlVkhMVFYWPj4/l/dqvXz/c3Nz45JNPilz7yy+/JCUlhVtuuaXc17/coEGDAJg2bVqR/R9++CHAVdcZOXIkcXFxfPHFF+zatatIUw/A7bffjsFg4PXXX7/qd1Ipdc2Zl63Fz8+Pfv368f333xdJeL/77jvS09OL1PyUVVRUFADr16+37MvIyLC8h8vjynvi7e1NgwYNrNJcLWxLmnuquV69evHoo48ydepUoqOj6d+/P66urhw5coSFCxfy8ccfc8cdd/DNN98wc+ZMbrvtNqKiokhLS+N///sfvr6+lg9hDw8PmjVrxvz582nUqBGBgYG0aNGCFi1alDmuZ599lgULFjBt2jTefvttAO699162bt3Kxx9/zP79+7n33nsJCAhgx44dfPXVVwQFBfHTTz8VmXV05MiR7NmzhzfffJOdO3dy9913W2acXbZsGatWrSpSE3KluXPn0qZNm6v6YJjdeuutPPHEE+zYsYN27drRpk0bxowZw8cff8yRI0e46aabAPjrr7/4448/GDNmTJHaqYCAABYtWsSgQYNo06ZNkRlnd+zYwY8//kjXrl2vea+6devGjBkzGDduHE2aNCky4+zatWv59ddfLYnOkCFD6NOnDy+++CKxsbG0bt2aFStWsGTJEp566inLF4S1hIeH88477xAbG0ujRo2YP38+0dHRfP7555b/p8GDB/PLL79w2223ccsttxATE8Ps2bNp1qxZuWvADh8+TN++fRkxYgTNmjXDxcWFRYsWkZCQwF133QVoTZmTJk3i9ddfZ8CAAdx6660cOnSImTNn0rFjxyKdZCuidevWPPDAA3z++eckJyfTq1cvtmzZwjfffMOwYcPo06dPkeMHDRqEj48PEydOxGAwFEm6QfsSf+ONN5g0aRKxsbEMGzYMHx8fYmJiWLRoEWPHjmXixInljtf8Xtm3bx+gJR5///03AC+99JLluDfffJNu3brRq1cvxo4dy+nTp/nggw/o378/AwYMKPf1+/fvT506dXj44Yd59tlnMRgMfPXVVwQHB3Py5MlyldmsWTN69+5N+/btCQwMZNu2bfz00088/vjj5Y5T2IkDRhQJByppxtnPP/9ctW/fXnl4eCgfHx/VsmVL9dxzz6m4uDillFI7duxQd999t6pTp45lwrfBgwerbdu2FSnn33//Ve3bt1dubm7lnszNrHfv3srX11clJycX2b948WJ10003qYCAAGU0GlWDBg3UM888o86fP1/itVatWqWGDh2qQkJClIuLiwoODlZDhgxRS5YsKfGc7du3K0C9/PLLJR5jnsPi6aeftuwrKChQH3/8sWrdurVyd3dX7u7uqnXr1uqTTz4pcUbNuLg49fTTT6tGjRopd3d35enpqdq3b6/efPPNEievKy7ee+65R4WHhytXV1cVEBCg+vbtq7755psi101LS1NPP/205biGDRteczK3y10+odrlivu/LG4yt7p166rp06cXOddkMqm33npL1a1bVxmNRtW2bVu1dOlS9cADD6i6dete99qXP2cegpyYmKjGjx+vmjRpory8vJSfn5/q3LmzWrBgwVXnTp8+XTVp0kS5urqq0NBQ9X//938lTuZ2pStjLEleXp56/fXXVWRkpHJ1dVURERFXTeZ2uXvvvdcyf0tJfv75Z9WjRw/l5eWlvLy8VJMmTdT48ePVoUOHrhv3tVDMpIzmx5U2bNigunXrptzd3VVwcLAaP358qSZOvN7v/vbt21Xnzp2Vm5ubqlOnjvrwww+vOZnblXr16qV69epl+fmNN95QnTp1Uv7+/srDw0M1adJEvfnmm8XOQi2ci06pcvR0E0KI6+jduzeJiYns3bvX0aEIISop6ZMihBBCCKckSYoQQgghnJIkKUIIIYRwStInRQghhBBOSWpShBBCCOGUJEkRQgghhFNyusncTCYTcXFx+Pj4WG2adSGEEELYllKKtLQ0wsPDr1oktrycLkmJi4srcXZPIYQQQji3U6dOUbt2bauU5XRJinkJ+VOnTlmWoRdCCCGEc0tNTSUiIsLyPW4NTpekmJt4fH19JUkRQgghKhlrdtWQjrNCCCGEcEqSpAghhBDCKUmSIoQQQgin5HR9UoQQQlR9Siny8/MpKChwdCiiDFxdXTEYDHa7niQpQggh7Co3N5ezZ8+SmZnp6FBEGel0OmrXro23t7ddridJihBCCLsxmUzExMRgMBgIDw/Hzc1NJu6sJJRSnD9/ntOnT9OwYUO71KhIkiKEEMJucnNzMZlMRERE4Onp6ehwRBkFBwcTGxtLXl6eXZIU6TgrhBDC7qw1bbqwL3vXesm7RAghhBBOSZIUIYQQQjglSVKEEEII4ZQkSRFCCCFKKT4+nieffJIGDRrg7u5OaGgo3bt3Z9asWUWGVNerV49p06YV+Vmn07Fp06Yi5T311FP07t27xOvFxsai0+mIjo4udYw6nc7ycHFxoU6dOkyYMIGcnJxSl+EsZHSPEMIhzqflsCT6DGeSsyz7Qn3dGdamFmF+7g6MTIjiHT9+nO7du+Pv789bb71Fy5YtMRqN7Nmzh88//5xatWpx6623lni+u7s7zz//POvWrbN5rHPmzGHAgAHk5eWxa9cuHnzwQby8vJgyZYrNr21NkqQIIezqn6OJfLfxBCsPJJBvUlc9/+6yg/RpHMJ9XerSu3GwzKFRDSilyMpzzMyzHq6GUr/Hxo0bh4uLC9u2bcPLy8uyv379+gwdOhSlrn4/X27s2LHMnj2bP/74g0GDBlUo7uvx9/cnLCwMgIiICIYOHcqOHTtsek1bkCRFCGEXufkm3vx9P99sPGHZ17aOP13qB6HXgVKw7cRFtsRcYNXBc6w6eI472tfmjWEtcHe13zTcwv6y8gpo9spyh1x7/+Sb8XS7/ldhUlISK1as4K233iqSoFzueslOZGQkjz32GJMmTWLAgAF2G4Z9+PBhVq9ezejRo+1yPWuSJEUIYXMJqdmMm7uD7ScuAnBP5zrc37UuTcJ8rzr26Ll05m4+wTf/xvLT9tMcjE9l1r3tiQiUib+E4xw9ehSlFI0bNy6yv0aNGmRnZwMwfvx43nnnnWuW89JLLzFnzhzmzp3LqFGjbBbv3XffjcFgID8/n5ycHAYPHsykSZNsdj1bkSRFCGFThxPSuPeLzZxPy8HH3YWPRrShX7PQEo9vEOLNq0Oa069pKE/8uJO9Z1IZMv1vvn2oE61q+9svcGE3Hq4G9k++2WHXrogtW7ZgMpm49957S9UxNTg4mIkTJ/LKK68wcuTICl37Wj766CP69etHQUEBR48eZcKECYwaNYp58+bZ7Jq2IEmKEMJmzqfl8OCcrZxPy6FRqDefjepAZI3iq8qv1L1BDX57ogf/9/12dp9O4aGvt7F4fDdqB0iNSlWj0+lK1eTiSA0aNECn03Ho0KEi++vXrw+Ah4dHqcuaMGECM2fOZObMmVaN8XJhYWE0aNAAgMaNG5OWlsbdd9/NG2+8YdlfGcgQZCGETWTlFjDm222cSc4isoYX88d2LXWCYlbL34O5YzrTJMyHxPQcHv56G6nZeTaKWIiSBQUFcdNNNzF9+nQyMjIqVJa3tzcvv/wyb775JmlpaVaK8NrM6+xkZWVd50jnIkmKEMLqTCbFhAXR7DqVjL+nK1+N7kiAl1u5yvJxd+XL0R0J9jFyKCGNx3/YSX6BycoRC3F9M2fOJD8/nw4dOjB//nwOHDjAoUOH+P777zl48GCZFtwbO3Ysfn5+/PDDD6U6/tChQ0RHRxd55OXlsWXLFpo0acKZM2eKHJ+cnEx8fDxxcXGsW7eOyZMn06hRI5o2bQpQ4nnOxrnr14QQldLs9cf4c288rgYdn93Xvsw1KFeq5e/Blw90YMRnG1l/+DzvrzjMfwc2sVK0QpROVFQUO3fu5K233mLSpEmcPn0ao9FIs2bNmDhxIuPGjSt1Wa6urkyZMoV77rmnVMffddddV+07deoUmZmZHDp0iLy8ojWMDz74IKA1pYWFhdGzZ0/eeustXFy0r/2SznM2OnW9gd12lpqaip+fHykpKfj6Xt3zXwjh3I6eS2fQxxvILTDxzvCWjOxYx2pl/777LON/2IFeB4vHd5eOtJVQdnY2MTExREZG4u4uk/ZVNtf6/7PF97c09wghrKbApHj+593kFpjo3TiYER0irFr+La1qMrhVTUwKnvtpN7n50uwjRFUmSYoQwmq+2xjL9hMX8XIz8NZtLW0yW+xrtzYnwNOVg/FpfLbumNXLF0I4D0lShBBWcepCJu8u14Zn/ndQU8L9Sz8ksyxqeBt5dUhzAD5dfZQjCfYZHSGEsD9JUoQQVjFl6X4ycwvoFBnIvZ2s1w+lOEPbhHNjkxByC0y8smTfdddMEUJUTpKkCCEqbMfJi6zYn4BeB2/d1gK93raLAup0OiYPbY6bQc/G40n8fTTRptcTQjiGJClCiApRSvHusoMA3NG+Ng1CfOxy3doBntzXpS4A7y47JLUpQlRBkqQIISpkw5FENh2/gJtBz5P9Gtn12uP7ROHlZmDPmRT+3Btv12sLIWxPkhQhRLmZTIr3CjvL3telLrVs1Fm2JEHeRsbcoK2d8v6KQzITrRBVjCQpQohy+3NvPHvOpODlZmB8nyiHxDDmhkgCPF05fj6Dn3ecdkgMQgjbkCRFCFEuSik+XnUYgDE31CfI2+iQOHzcXRnfR1vV9ZNVR6U2RVQpa9euRafTkZycbLdrjh49mmHDhtntetciSYoQolzWHj7P4YR0vI0uPNQj0qGx3NelLkFebpxJzpK+KcKm4uPjefLJJ2nQoAHu7u6EhobSvXt3Zs2aRWZmpuW4evXqMW3atCI/63Q6Nm3aVKS8p556it69e9sp+spHkhQhRLl8seE4ACM7RuDn4erQWNxdDYzqqo30+WLDcRnpI2zi+PHjtG3blhUrVvDWW2+xc+dONm7cyHPPPcfSpUtZuXLlNc93d3fn+eeft1O0VYMkKUKIMtsXl8I/R5Mw6HU82L2eo8MBYFSXuhhd9Ow6ncLW2IuODkeUhVKQm+GYRxkS2nHjxuHi4sK2bdsYMWIETZs2pX79+gwdOpTff/+dIUOGXPP8sWPHsmnTJv74448K3a6ff/6Z5s2bYzQaqVevHh988EGR53Nycnj++eeJiIjAaDTSoEEDvvzySwAKCgp4+OGHiYyMxMPDg8aNG/Pxxx9XKB5bcnF0AEKIyufLDTEADGwRRu0ATwdHownyNnJ7u9r8uOUk/9twnE6RgY4OSZRWXia8Fe6Ya78QB25e1z0sKSnJUoPi5VX88ddbqyoyMpLHHnuMSZMmMWDAAPT6stcTbN++nREjRvDaa68xcuRI/v33X8aNG0dQUBCjR48G4P7772fjxo188skntG7dmpiYGBITtQkPTSYTtWvXZuHChQQFBfHvv/8yduxYatasyYgRI8ocj61JkiKEKJOzKVn8uisOgEcKh/86i4d7RPLjlpOsPJDA8fPp1A/2dnRIooo4evQoSikaN25cZH+NGjXIzs4GYPz48bzzzjvXLOell15izpw5zJ07l1GjRpU5jg8//JC+ffvy8ssvA9CoUSP279/Pe++9x+jRozl8+DALFizgr7/+ol+/fgDUr3/p99TV1ZXXX3/d8nNkZCQbN25kwYIFkqQIISq/r/+NJd+k6FQvkNYR/o4Op4gGId70bRLCqoPn+PLvGN68raWjQxKl4eqp1Wg46toVsGXLFkwmE/feey85OTnXPT44OJiJEyfyyiuvMHLkyDJf78CBAwwdOrTIvu7duzNt2jQKCgqIjo7GYDDQq1evEsuYMWMGX331FSdPniQrK4vc3FzatGlT5ljsQfqkCCFKLTuvgB83nwS0+UmckXlyt593nCYlK8/B0YhS0em0JhdHPK7TRGPWoEEDdDodhw4dKrK/fv36NGjQAA+P0k9kOGHCBLKyspg5c2aZblNpXC+OefPmMXHiRB5++GFWrFhBdHQ0Dz74ILm5uVaPxRokSRFClNofe86Smp1PLX8P+jYNdXQ4xepSP5DGoT5k55lYEn3G0eGIKiIoKIibbrqJ6dOnk5GRUaGyvL29efnll3nzzTdJS0sr07lNmzbln3/+KbLvn3/+oVGjRhgMBlq2bInJZGLdunXFnv/PP//QrVs3xo0bR9u2bWnQoAHHjh0r92uxNUlShBClNm/LKUAbdmyw8UrH5aXT6birUwQAP245JcORhdXMnDmT/Px8OnTowPz58zlw4ACHDh3i+++/5+DBgxgMhlKXNXbsWPz8/Pjhhx/KFMMzzzzDqlWrmDJlCocPH+abb75h+vTpTJw4EdDmY3nggQd46KGHWLx4MTExMaxdu5YFCxYA0LBhQ7Zt28by5cs5fPgwL7/8Mlu3bi1TDPYkSYoQolSOnktnS+wF9Dq4s0NtR4dzTbe1rYWbi54DZ1PZfTrF0eGIKiIqKoqdO3fSr18/Jk2aROvWrenQoQOffvopEydOZMqUKaUuy9XVlSlTplg63ZZWu3btWLBgAfPmzaNFixa88sorTJ482TKyB2DWrFnccccdjBs3jiZNmvDII49Yan8effRRbr/9dkaOHEnnzp1JSkpi3LhxZYrBnnTKyf7MSE1Nxc/Pj5SUFHx9fR0djhCi0Ju/7+d/G2Lo2ySEL0d3dHQ41/XUvJ0sjo7j7k4RTL29laPDEYWys7OJiYkhMjISd3d3R4cjyuha/3+2+P6WmhQhxHXl5Bfw8w6tf8ddneo4OJrSMcf5a3QcGTn5Do5GCFEekqQIIa7rr/0JXMjIJdTXSJ/GwY4Op1Q6RwZSv4YXGbkF/LbLQcNbhRAVIkmKEOK6zB1mR3SIwMVQOT42dDodIzsWdqDdesrB0QghyqNyfNoIIRzmTHIWfx9NRKfTkpTKZHj72rgadOw6lczhhLIN9RRCOJ4kKUKIazLPNdI5MpCIQOdYp6e0angb6dUoBIDFO2XOFGfiZGM2RCnZ+/9NkhQhxDUt2an15xjWppaDIymfYW21heuWRMdhMskXo6O5uroCkJmZ6eBIRHmYZ6Yty5wwFSFr9wghSnTgbCqHEtJwM+gZ2LKmo8Mpl35NQ/E2unAmOYvtJy/SsZ6sjuxIBoMBf39/zp07B4Cnp+d1Vw8WzsFkMnH+/Hk8PT1xcbFP+iBJihCiREuitVqUPk2C8fNwdXA05ePuauDm5mH8vOM0S6LPSJLiBMLCwgAsiYqoPPR6PXXq1LFbYlmmJGXq1Kn88ssvHDx4EA8PD7p168Y777xTZOnq3r17X7VmwKOPPsrs2bOtE7EQwi5MJsWvhf1RKmtTj9mwtuH8vOM0v+8+y6tDmuNaSUYoVVU6nY6aNWsSEhJCXp4sAlmZuLm5odfb7/enTEnKunXrGD9+PB07diQ/P58XXniB/v37s3//fry8vCzHPfLII0yePNnys6dn5epsJ4SArbEXiEvJxsfoQp8mIY4Op0K61g+ihreRxPQc1h8+77SLI1Y3BoPBbn0bROVUpiRl2bJlRX7++uuvCQkJYfv27fTs2dOy39PT01KdJ4SonBYXNvUMaBGGu2vl/iJxMegZ0romc/6JZXF0nCQpQlQSFaqzSUnRFu4KDCzaxjt37lxq1KhBixYtmDRp0jV7cefk5JCamlrkIYRwrNx8E3/sOQvAsLaVu6nHzNxk9df+eNJlmnwhKoVyJykmk4mnnnqK7t2706JFC8v+e+65h++//541a9YwadIkvvvuO+67774Sy5k6dSp+fn6WR0RE5ZosSoiqaMOR86Rk5RHiY6RL/SBHh2MVrWr7EVnDi+w8E6sOJDg6HCFEKZR7dM/48ePZu3cvf//9d5H9Y8eOtWy3bNmSmjVr0rdvX44dO0ZUVNRV5UyaNIkJEyZYfk5NTZVERQgH+3NvPAADW4Rh0FeN4aE6nY5bWtZk+pqj/LknnqGVvDOwENVBuWpSHn/8cZYuXcqaNWuoXbv2NY/t3LkzAEePHi32eaPRiK+vb5GHEMJx8gpM/LVfq2morHOjlGRAC62v3NrD58jMlSYfIZxdmZIUpRSPP/44ixYtYvXq1URGRl73nOjoaABq1qxaH3ZCVFUbjyWRkpVHDW+3KjenSPNwXyICPcjOM7H20HlHhyOEuI4yJSnjx4/n+++/54cffsDHx4f4+Hji4+PJysoC4NixY0yZMoXt27cTGxvLr7/+yv3330/Pnj1p1aqVTV6AEMK6zE09NzWrOk09ZjqdjkEttD+YzK9TCOG8ypSkzJo1i5SUFHr37k3NmjUtj/nz5wPaJC8rV66kf//+NGnShGeeeYbhw4fz22+/2SR4IYR1FZgUK/ZpX96DWlbNaQTMTT6rDySQnVfg4GiEENdSpo6z11v9MCIi4qrZZoUQlceWmAskZeTi5+FaZUb1XKlNhD/hfu7EpWSz4UgiNzWTOVOEcFYyN7QQwmLZXm1ulP7NQqvs1PE6nY6bC2tT/ix8vUII51Q1P4WEEGVmMqlLQ4+raFOP2aDCUUt/7U8gN9/k4GiEECWRJEUIAcDOUxc5l5aDj9GF7g1qODocm2pfJ4BgHyNp2fn8eyzR0eEIIUogSYoQAoAVhXOj9GkSgtGlcq/Vcz16vY6bm2t9UcxzwgghnI8kKUIIAFYWfllXl46k/QoXGVx14Nx1BwUIIRxDkhQhBDGJGRw7n4GLXkevxsGODscuutQPwtPNQHxqNvviZGFTIZyRJClCCMuCe53rB+Lr7urgaOzD3dVAz4ZaQiZNPkI4J0lShBCsLExSzE0g1UW/wqatlbIqshBOSZIUIaq55MxctsZeBKpfktKncTA6HeyLSyUuOcvR4QghriBJihDV3NpD5ykwKRqH+hAR6OnocOwqyNtI+zoBAKw6eM7B0QghriRJihDVnKWpp1mIgyNxjL6FtUcrpV+KEE5HkhQhqrHcfBPrDp0Hql9Tj9lNhcnZxmNJpOfkOzgaIcTlJEkRohrbGnuBtJx8angbaV3b39HhOERUsDf1gjzJLTDx95Hzjg5HCHEZSVKEqMbMTT19m4Sg1+scHI1j6HS6S00+B6RfihDORJIUIaoxc1NPnybVYwK3kvRprDX5rDt8HpNJZp8VwllIkiJENXUiKYPjidoss1V9QcHr6RgZgKebgfNpOew/K7PPCuEsJEkRoppaW1iL0qFeAD7VZJbZkhhdDHSL0hK1tYekyUcIZyFJihDV1JrCL+Pejavn0OMr9S5cs8icvAkhHE+SFCGqoey8AjYeSwIu9ceo7sxJyo6TF0nOzHVwNEIIkCRFiGpp0/EkcvJN1PRzp1Got6PDcQq1AzxpGOKNScGGI4mODkcIgSQpQlRL5iaN3o1D0Omq59Dj4vRpotUqSZOPEM5BkhQhqqG1lv4o1Xvo8ZV6N9Lux7rD52QoshBOQJIUIaqZmMQMYpMycTXI0OMrdagXiJebgcT0XPbFyVBkIRxNkhQhqhlzLUrHeoF4G10cHI1zcXPRWxK3NTIUWQiHkyRFiGpm3WFzfxRp6ilO78tmnxVCOJYkKUJUIzn5BWw6rg097tlIkpTi3NBQq0mJPpVManaeg6MRonqTJEWIamRb7EWy80wE+xhpHOrj6HCcUkSgJ/VreFFgUvx7NMnR4QhRrUmSIkQ1sv6I1oRxQ8MaMvT4Gsy1KRuOSJOPEI4kSYoQ1ciGw9okZT0bSlPPtZibwmRSNyEcS5IUIaqJy1f47dFQhh5fS5f6QbgadJy8kMmJpAxHhyNEtSVJihDVxD9HtVqB5uG+1PA2Ojga5+ZldKFdnQAA1ssoHyEcRpIUIaqJS/1RpKmnNMxNPuulyUcIh5EkRYhqQCll6V/RU5p6SsXcb2fjsSTyCkwOjkaI6kmSFCGqgUMJaZxPy8HdVU/7egGODqdSaB7uS4CnK+k5+USfSnZ0OEJUS5KkCFENmPtVdKkfhNHF4OBoKge9XkePwtoU6ZcihGNIkiJENWBu6pH+KGVjni9F+qUI4RiSpAhRxWXnFbAl5gJw6UtXlI75fu05nUxKlkyRL4S9SZIiRBW348RFcvJNhPgYaRji7ehwKpWafh7UD/bCpLCseSSEsB9JUoSo4v45pjVVdG8gU+GXR48GWm2KeZ4ZIYT9SJIiRBX3d+Eied0bSFNPeZjv29+SpAhhd5KkCFGFpWTmsed0MgDdGwQ5NphKqkv9IPQ6OH4+g7jkLEeHI0S1IkmKEFXYxuNJmBREBXtR08/D0eFUSn4errSq7Q9Ik48Q9iZJihBVmPlLtYc09VSIuRbq32PSeVYIe5IkRYgqzJykSH+Uirm8X4pSysHRCFF9SJIiRBUVl5zF8cQM9DroEiX9USqiXZ0A3F31nE/L4ci5dEeHI0S1IUmKEFWUuRalVW1/fN1dHRxN5ebuaqBjvUAA/pbZZ4WwG0lShKiipD+KdZmbfP49JkmKEPYiSYoQVZBSin8KO3l2k6HHVmFO9jYdv0BegcnB0QhRPZQpSZk6dSodO3bEx8eHkJAQhg0bxqFDh4ock52dzfjx4wkKCsLb25vhw4eTkJBg1aCFENd29Fw659NyMLroaV83wNHhVAnNavri5+FKek4+e86kODocIaqFMiUp69atY/z48WzatIm//vqLvLw8+vfvT0ZGhuWYp59+mt9++42FCxeybt064uLiuP32260euBCiZOahsh3rBWJ0MTg4mqpBr9fRtb5WK7VRhiILYRcuZTl42bJlRX7++uuvCQkJYfv27fTs2ZOUlBS+/PJLfvjhB2688UYA5syZQ9OmTdm0aRNdunSxXuRCiBKZ+01IU491dW8QxLJ98fxzNJHxfRo4OhwhqrwK9UlJSdGqPAMDtV7v27dvJy8vj379+lmOadKkCXXq1GHjxo3FlpGTk0NqamqRhxCi/ApMyvKXfrco6TRrTV0L7+e2ExfJzitwcDRCVH3lTlJMJhNPPfUU3bt3p0WLFgDEx8fj5uaGv79/kWNDQ0OJj48vtpypU6fi5+dneURERJQ3JCEEsD8uldTsfHyMLrQI93V0OFVKVLAXIT5GcvNN7Dh50dHhCFHllTtJGT9+PHv37mXevHkVCmDSpEmkpKRYHqdOnapQeUJUd+amns71A3ExyAA+a9LpdHSLkn4pQthLuT7BHn/8cZYuXcqaNWuoXbu2ZX9YWBi5ubkkJycXOT4hIYGwsLBiyzIajfj6+hZ5CCHKz9xptqs09diEuQlNFhsUwvbK1HFWKcUTTzzBokWLWLt2LZGRkUWeb9++Pa6urqxatYrhw4cDcOjQIU6ePEnXrl2tF7UQoli5+Sa2xl4ALi2K51B5WXBuP8TvgfTzRZ/T6SAwEsJaQWB90FeOUUjmzsi7TqeQnpOPt7FMH6NCiDIo02/X+PHj+eGHH1iyZAk+Pj6WfiZ+fn54eHjg5+fHww8/zIQJEwgMDMTX15cnnniCrl27ysgeIexg9+lkMnMLCPJyo1GIj/0DyM+Fw8vg4FI4uwsSD4MqxcRnrl4Q2hxqd4CWd0J4Wy2JcUK1AzypE+jJyQuZbI25QJ8mIY4OSYgqq0xJyqxZswDo3bt3kf1z5sxh9OjRAHz00Ufo9XqGDx9OTk4ON998MzNnzrRKsEKIazM39XSJCkKvt9OXvFJwNhqif4A9P0HWhaLPe9aAsJbgHwFcFlNBnpbEJOyDvAw4vUV7bJoJwU2gzT3QaiT4FN9U7EjdooI4eSGTf48lSpIihA2Vubnnetzd3ZkxYwYzZswod1BCiPIx95PoZq9Vj4+tgZWvarUmZj41tdqQejdoyYlP2LVrRUwFkHQM4ndrtTAHfoPzB+GvV2Dla9D8duj7CgTUtfnLKa2uUUHM23rKkhQKIWxDGlOFqCKy8wrYeTIZsMP8KOcOwIqX4ehf2s8GIzQdrNV+1O9Ttv4legMEN9IeLe+A7BTYt0irmTm1Gfb+BAd+hc6PwQ3PgIe/TV5SWXQtTAL3n03lYkYuAV5uDo5IiKpJxicKUUVsP3GR3AITNf3cqRfkaZuLZCTCb0/CrG5agqJ3gc7/BxMOwB1fQYN+Fe8A6+4H7UfDwyvg0fUQ2RMKcuHfT+CTtrD5M632xYFCfNxpGOKNUrA5RmpThLAVSVKEqCLM83Z0rR+EzhadTo+v1ZKT7V9rnWGbDoHxW2Dg2+Blo+almq3h/l/hnoVaP5WsC/Dnc/DtUEiNs801S6mrzJcihM1JkiJEFWGexK2LtfujFOTBqsnw7TBIT9CShQeXwcjvISjKutcqjk4HjfrDY//ALR9oI4FiN8Cs7nBo2fXPtxHzYoPSL0UI25EkRYgqICMnn92ntbW0zF+eVpF8EuYMgg0fAEprhnlkDdR1wLxHBhfoOAYe26DVsGRdgB9Hwp//hfwcu4fTufA+HzmXzvk0+19fiOpAkhQhqoCtsRfINylqB3gQEWil/iix/8DsG7RhwUY/uPNrGPIxuNmov0tpBUXBw39Bl3Haz5tnaYlUhn1ngA30cqNJmDYXzabjUpsihC1IkiJEFbDxuHnVYyvVouz/Fb67DbKToVZ7rfai+W3WKdsaXIwwYCrcswA8AuDMNviyP1yMtWsY5lFUGyVJEcImJEkRogqwdJq1RpKy9QtYcD8U5ECTwTD6d6eao6SIRjdrtSp+deDCMS1RObvbbpc33+9N0i9FCJuQJEWISi41O4+9Z8z9USowP4pSsPpN+P0ZtP4nD8KIb8HVwzqB2kqNhtpw5dAWWsfeOYPg+Dq7XLpTZCB6HRxPzCAhNdsu1xSiOpEkRYhKbsvxC5gURNbwIszPvXyFKAV/TIT172o/934BBn9UaRb9w7cmPPiHNsttbhp8PxwO/Wnzy/p5uNI83A+QochC2IIkKUJUcub+EF3KO6pHKW0K+q1fADoYPA16P++0C/yVyN0P7v0Jmg0FUx4seECb28XGZL4UIWxHkhQhKrkK90dZ/742myvArZ9AhwetFJkDuLrD8K+0vjQFOfDjPXBqi00vaZkv5bh9RxcJUR1IkiJEJXYxI5f9Z1OBcs6PsmkWrHlD2755KrS734rROYjBRZuiv34fbXXluXfYtDNtx8hADHodpy5kcfpips2uI0R1JEmKEJXY5pgLADQI8SbYx1i2k3d8B8v+q233fgG6jrNydA7kYoS75kJEF23Bwu9ug8QjNrmUt9GFVrW1fimbjl+wyTWEqK4kSRGiEjNPIlbmWpRDy+C3/2jbXR+HXs9ZOTIn4OYF9y7QZqfNTNSm9U9LsMmlzP2BpF+KENYlSYoQlVi5+qMk7IefH9YWCWx7H/R/o/J1ki0tdz+4bxEENYTU0zD/Xsiz/lBhc5K46XgSSimrly9EdSVJihCVVFJ6DocS0gDoHBlYupMyErX1bnLTteG6g6dV3QTFzCsI7pkP7v5weqtWg2TlRKJDvQBc9DrOJGdx+mKWVcsWojqTJEWISsrcH6VxqA9B3qXoj5KfC/NHaYsGBtTTJmozuNo2SGcRFAUjvgGdAXbPh3+mWbV4TzcXWkf4A9LkI4Q1SZIiRCVVpqYepeD3CXDyXzD6wt3zwbOUtS9VRf3eMPAdbXvl63DwD6sWb27ykXV8hLAeSVKEqKQ2lWUSt02zYOd3oNNrw3NDmtg4OifV6RHoOAZQ8PMYSNhntaIt6/hIvxQhrEaSFCEqofNpORw5l45OV4r+KCc3w4qXtO2bpkDDm2wfoDMb8DZE9tLmUFlwP+SkWaXYdnUCcDXoOJuSzYkkmS9FCGuQJEWISshci9IkzJcAL7eSD8y8AD89BKoAmt8OXcfbKUInZnCFO78G31qQdBSWTrBKR1oPNwNtIwIAafIRwlokSRGiEirV/ChKweJx2tDbwPow5OOqP5KntDwDYfiXWkfaPQu0pjAr6HJZk48QouIkSRGiErq0qOA1mno2zoDDf4LBTas5cPe1T3CVRd2ucGNhM9gfz1qlf4r5/2PjMemXIoQ1SJIiRCWTkJrN8fMZhf1RSqhJOb0NVr6qbd/8ljbrqrha96egQT/Iz4aFoyEnvULFtasTgJuLnnNpORxPzLBKiEJUZ5KkCFHJmJsSmtX0xc+zmHlOsi7CwgfBlA/NhhaOZhHF0uvhts/ApyYkHoY/JlaoOHdXA21lvhQhrEaSFCEqmev2R/njWUgpnLDt1k+lH8r1eNUo7J+ih10/wt5fKlRcV+mXIoTVSJIiRCVjXmm32Enc9i2CPQu1L9zhX2pr14jrq9cdbiisRfl9AqTFl7uoS+v4XJB+KUJUkCQpQlQi8SnZxCRmoNdBxyvnR0lL0IbTAvSYALU72D/AyqznsxDWSmsu+/WJcg9LblPHH6OLnsT0HI6dr1gfFyGqO0lShKhENh5PBKBFLT983S/rj6KUtnBe1gUIawm9nndQhJWYi5vWP8XgBkdWwI5vy1WM0cVA+7qF86VIvxQhKkSSFCEqkU3HCpt6ruyPsvN7OLxM+4K97XPtC1eUXWgzuPFlbXv5C3AxtlzFXN7kI4QoP0lShKhENha3Xs/FE7Bskrbd50Xti1aUX9fxUKcb5KbD4vFgMpW5iC6yjo8QViFJihCVxJnkLE5eyMSg113qj6IULBkPuWkQ0QW6PeHYIKsCvQGGzQRXLzjxN2yeXeYiWtf2x8PVQFJGLocTpF+KEOUlSYoQlcSmwv4NLWv54W100Xbu+BZiN4CrJ9w2S/uCFRUXGAk3v6Ftr55S5mYfNxc9Hepp/VJkKLIQ5SdJihCVxFVNPWnx8Fdh/4kbX9LW5xHW0/5BqNsD8jLLtQih+f9JOs8KUX6SpAhRSZi/7Czzo/z5HGSnQHhb6PyYAyOronQ6bVFGgxGOrYLdC8p0ujlJ2RSThMkk/VKEKA9JUoSoBE5dyORMchYueh0d6gbAwd9h/xJtFd9bP5VmHlup0QB6PadtL58EGaWvFWlV2w9PNwPJmXkcSkizUYBCVG2SpAhRCZibelrV9sNLZcLvhbOjdv+PNi+KsJ3uT0JIc8hM0oYll5KrQU+HepdWRRZClJ0kKUJUApsub+pZ9TqkxWl9UGTSNtszuGq1Vehg9zw4urLUp5rnS9konWeFKBdJUoRwckopy5dcf+9Y2PqF9sTgaeDq4bC4qpXa7S/1+1n6NORmlOo0c/+hzceTKJB+KUKUmSQpQji5E0mZnE3Jxt1gomX069rONvdB/V6ODay6ufEl8IuA5JOw4YNSndIi3Bdvowup2fkcOJtq4wCFqHokSRHCyZlrUZ4P/Bv9+f3gEQA3TXZwVNWQ0RsGvqNt//MJJB697ikuBj2dIqVfihDlJUmKEE7u32NJBHORezK/03b0fRW8gq59krCNxoOgYX8w5cEfE0s1d0q3wiaff48l2jo6IaocSVKEcGJKKTYeS+IF1x8wFmRAeDtod7+jw6q+dDqtNsVghONrtGHg12GeL2Vr7EXyC8q+DpAQ1ZkkKUI4sWPn04nKiOY2wz8odHDLBzIniqMF1oceT2vby1+AnGuvzdOspi9+Hq6k5+Sz50yKHQIUouqQJEUIJ7bpSDyTXecAoOvwINRq5+CIBAA9ngL/upB6Bta/e81D9Xodnc39UmQoshBlIkmKEE7MY+cXNNafJsvFH2582dHhCDNXDxhYmJxsnAHnD13zcPNQZOk8K0TZSJIihJMypcQxIPFrAOI7TwLPQMcGJIpqPEDrSGvKhz+evWYnWnOSsi32Irn50i9FiNKSJEUIJ5X2+8t4kc0u1YBavcc4OhxRnAFTtU60Meu09ZRK0CjEhyAvN7LyCth9Otl+8QlRyZU5SVm/fj1DhgwhPDwcnU7H4sWLizw/evRodDpdkceAAQOsFa8Q1cOZ7fgd/gmARaFP4Obq4uCARLEC6kG3J7TtFS9Cfk6xh+n1OssoH2nyEaL0ypykZGRk0Lp1a2bMmFHiMQMGDODs2bOWx48//lihIIWoVpSCP/8LwM8FPQhp1sPBAYlr6vE0eIfBxVjYNLPEw7pY5kuRJEWI0irzn2cDBw5k4MCB1zzGaDQSFhZW7qCEqNb2/ASnt5CJkXfz7uKzqBqOjkhci9EbbnodFj0K69+H1neDz9Wff+ZJ3bafvEh2XgHurjKUXIjrsUmflLVr1xISEkLjxo35v//7P5KSSv7LIScnh9TU1CIPIaqt3Az46xUAZuQNJdMYQotwXwcHJa6r5Qio1R5y02HVlGIPqV/Di1BfI7n5JnacuGjnAIWonKyepAwYMIBvv/2WVatW8c4777Bu3ToGDhxIQUFBscdPnToVPz8/yyMiIsLaIQlRefzzMaTFkeoezhcFg+hcPxAXg/Rvd3p6PQwoXNcn+ns4s/2qQ3Q6Hd0Ka8WkyUeI0rH6p99dd93FrbfeSsuWLRk2bBhLly5l69atrF27ttjjJ02aREpKiuVx6tQpa4ckROWQfFJLUoCvvceQgxtdpamn8ojoCK3u0raXTSp2SHJXWcdHiDKx+Z9o9evXp0aNGhw9WvyKoUajEV9f3yIPIaqlla9BfjamOt2ZldAMgO4NZCHBSqXfq+DqCac2w96fr3ra3C9l1+kU0nPy7R2dEJWOzZOU06dPk5SURM2aNW19KSEqr9PbCr/UdOxv/QJZeSaCvNxoFOLj6MhEWfiGX1rXZ+XrkJdd5OnaAZ7UCfSkwKTYGnPBAQEKUbmUOUlJT08nOjqa6OhoAGJiYoiOjubkyZOkp6fz7LPPsmnTJmJjY1m1ahVDhw6lQYMG3HzzzdaOXYiqQSlY/qK23eYeVl0MBbQhq3q9zoGBiXLp+jj4hEPKSdjy2VVPd5MmHyFKrcxJyrZt22jbti1t27YFYMKECbRt25ZXXnkFg8HA7t27ufXWW2nUqBEPP/ww7du3Z8OGDRiNRqsHL0SVcOBXOLUJXDzgxpcsX17mLzNRybh5Qt/CdZbWfwAZRTvJdpX5UoQotTLPk9K7d2/UNdaoWL58eYUCEqJayc+Fv17Vtrs9QZZ7KDtP7tJ+lE6zlVeru7SJ3eL3wLp3YNCllZLNScr+s6lczMglwMvNUVEK4fRkbKMQjrT1C7gYA14h0P1Jtp+4SG6BiZp+7tQL8nR0dKK89Hro/6a2ve1LSLw0cCDEx52GId4oBZtjpDZFiGuRJEUIR8m8oP2VDXDji2D0tjT1dI0KQqeT/iiVWv1e0PBmbZXkla8WeaqbNPkIUSqSpAjhKOvfh+xkCGkGbUcB8E/hl1bX+tIfpUroPwV0Bji4FGL/tuw2z3/zz1HpPCvEtUiSIoQjXIiBLZ9r2zdNAb2BlKw89pxOBqB7A+mPUiUEN4b2D2jbK14CkwnQklCdDo6dzyA+JfsaBQhRvUmSIoQjrJ4Cpjyo3wca9gNg8/EkTEpb4yXc38PBAQqr6f0CuHlD3E7YvwgAP09XWtbyA2QoshDXIkmKEPZ2Zodl4jZummzZba767yazzFYt3sHQ/Ulte9VkbUQXl2rL/pYmHyFKJEmKEPaklGWVY1qNgJqtLE+Z+6P0kKaeqqfrePAOhYuxsH0OAN3Niw0eTbrmtA5CVGeSpAhhT0dXQuwGMLhBnxctuxNSszl6Lh2dDrpIp9mqx80Lev9X2173DmSn0qFeAG4ueuJTszmemOHY+IRwUpKkCGEvpoJLE7d1GgsBdS1PmZt6WoT74e8pk3tVSW3vh6CGkJkE/3yMu6uBDnUDABnlI0RJJEkRwl52z4dz+8DoBzc8U+Spf45qTT0yqqcKM7hoqyQDbJwBqWct/9+SpAhRPElShLCHvGxYXTgD6Q0TwDPQ8pRSyvIl1V06zVZtTQZDRGfIz4K1Uy1JysZjSRSYpF+KEFeSJEUIe9jyGaSeBt9a0PnRIk8dT8wgPjUbN4OeDnUDSyhAVAk6nTYvDsDO72jpFo+Puwup2fnsPZPi2NiEcEKSpAhha1kXYcMH2nafF8G16Bwo/xbWorSvG4CHm8He0Ql7q9NZq1FRJgxrplhmF/5H5ksR4iqSpAhha39Pg+wUCG4Kre+6+mlp6ql++r4COj0cXMqwGmcA6ZciRHEkSRHCllLjYPNsbbvfq6AvWlNSYFJsLJwfpZt0mq0+ghtDm3sA6HN6JqDYGnuR7LwCx8YlhJORJEUIW1r7NuRnQ0QXaDTgqqd3n04mNTsfH3cXWhVOky6qid6TwGDEI24Tt3nvJzffxNbYC46OSginIkmKELaSeAR2fq9t3/S61mnyCn8fKZwKPyoIF4P8OlYrfrWh81gAnjXMQ4fJ8n4QQmjkU1EIW1k9BVQBNBoIdboUe8iGwn4INzQMtmdkwln0mABGP8JzjjFU/y8bJEkRoghJUoSwhTPbYf8SQKd1kixGRk4+O09eBOCGhtIfpVryDIQe2uKDz7gs5OjZJM6n5Tg4KCGchyQpQlibUrDyNW279d0Q2qzYwzbHJJFXoIgI9KBukJf94hPOpfP/gXcYEfrz3GNYxb8yFFkIC0lShLC2Y6shZn3hIoKTSjxs/WHty6hHA2nqqdbcPKH38wA84bKILQdPOjggIZyHJClCWJPJBKsma9sdx4B/nRIP/dvSH0Waeqq9tqPI9KlHkC6Nuke+RimZIl8IkCRFCOs6sATORoObz1WLCF7ubEoWR8+lo9dpI3tENWdwxaXfywDcnb+EmBMnHByQEM5BkhQhrKUgD1YVrsvS7QnwKrmGxDzUtGVtf/w93ewRnXBybi1vJ8a1AT66LDJXv+focIRwCpKkCGEt0XPhwjHwrAFdx13zUEtTj8wyK8z0evY1exqAxifnQ/IpBwckhONJkiKENeRlabPLAvR8Fow+JR5qMinLOi09pD+KuEy9joP5t6AZruRRsGaqo8MRwuEkSRHCGrZ8Dmlnwa8OdHjwmofuP5tKYnounm4G2tUJsFOAojJoFu7HZ673AaDf/SOcO+jgiIRwLElShKiorGTY8KG23WcSuBivefj6I+cBrcOsm4v8CopL9HodAY26saygIzpl0mYtFqIak09IISrq308gOxmCm0Crkdc9fN0hLUnp1UjmRxFX69U4mPfz76QAPRxcCqe3OTokIRxGkhQhKiItHjbN0rZvfBn0hmsenp6Tz/YT2lT4PSVJEcW4oWEwR1Vtfs6/Qdux8jVtFmMhqiFJUoSoiPXvQV4m1O4ITW657uH/Hk0k36SoF+QpU+GLYtXwNtKili/T8odToHeF2A3aLMZCVEOSpAhRXheOw/avte1+r4FOd91TzP1RpBZFXEvPhsHEUYP1fsO0Hate12YzFqKakSRFiPJa8xaY8qFBP6jX47qHK6VYd1j6o4jrM78/pqQMQLn5wNldsH+xY4MSwgEkSRGiPM7uhj0Lte2+r5TqlNikTE5dyMLVoKNLfZkKX5SsXd0AvI0uHM/0IKHFI9rO1W9osxoLUY1IkiJEeZiHhrYYDjVbl+qUdYfOAdCxXiBeRhdbRSaqAFeD3rKm0xL3odosxheOwc7vHRyZEPYlSYoQZRX7DxxZAXoX6PNiqU9bX7hej/RHEaVhfp+sPJ6pzWIMsO4dyM10YFRC2JckKUKUhVJaJ0aAdvdDUFSpTsvOK2DjsSRA+qOI0jG/T3acTCa1xX3abMZpZ7XZjYWoJiRJEaIsDi+DU5vBxQN6Plfq07bFXiQrr4AQHyNNwkpe10cIs4hAT+rX8KLApPgnJg36vKA98feHkHXRscEJYSeSpAhRWqYCWFlYi9LlMfCtWepTVx/U+qP0bhyMrhRDlYUA6N04BIA1h85BqxEQ3BSyU+Cfjx0cmRD2IUmKEKW1ez6cPwDu/tD9qTKduraw02yfwi8dIUqjTxOtyWfNofMonf7SSLJNsyH1rAMjE8I+JEkRojTysrV5UQBumAAe/qU+NTYxg+OJGbjodfRoWMM28YkqqVNkIJ5uBs6n5bAvLhUaD4SILpCfBevednR4QticJClClMa2LyHlFPjWgk5jy3Tq2suGHvu4u9oiOlFFGV0MdG+gJbZrD53TZjXu95r25I7vIPGo44ITwg4kSRHierJTYP372nbv/4KrR5lOX1246rG56l6IsjA3EZr7NVG3KzQaAKrg0nw9QlRRkqQIcT3/fgpZF6BGI2h9T5lOzczNZ9Nxbeix9EcR5dG7sZbc7jyVzIWMXG1n31cAnTZV/pkdDotNCFuTJEWIa0lLgI0ztO0bXwZD2WaK3Xgsidx8E7UDPGgQ4m2DAEVVF+7vQZMwH5SCDYULVBLaHFrfpW2vfM1hsQlha5KkCHEt69+FvEyo1QGaDinz6WsuG9UjQ49FefVpUjgU2dzkA9q8KQY3iFkHx1Y7KDIhbEuSFCFKknQMtn+tbfd7Teu0WAZKKdYclP4oouLMTYXrDp+nwKS0nf51oOMYbfuvV8FkclB0QtiOJClClGT1FDDlQ4ObIPKGMp9+5Fw6Z5KzMLro6Vpfhh6L8mtXxx9fdxcuZuYRfSr50hM3TASjL8Tvhr0/Oyw+IWylzEnK+vXrGTJkCOHh4eh0OhYvXlzkeaUUr7zyCjVr1sTDw4N+/fpx5MgRa8UrhH2c2Q77FgGXDfkso1UHtKr5LvWD8HAzWC82Ue24GPSWBQdXH0y49IRXEHR/UttePRnycxwQnRC2U+YkJSMjg9atWzNjxoxin3/33Xf55JNPmD17Nps3b8bLy4ubb76Z7OzsCgcrhF0opVWfA7S+G8JalKuYVQe0L5N+zUKtFZmoxvo11d5HK/efK/pEl3HgHQbJJ2Hrlw6ITAjbKXOSMnDgQN544w1uu+22q55TSjFt2jReeuklhg4dSqtWrfj222+Ji4u7qsZFCKd15C+I3QAG46VF3cooKT2H7Se1ReD6NpGhx6LiejcOxqDXcSghjVMXMi894eYJfSZp2+vf0+b1EaKKsGqflJiYGOLj4+nXr59ln5+fH507d2bjxo3FnpOTk0NqamqRhxAOYyqAlYW1KJ3Hgn9EuYpZc+g8SkHzcF/C/cs2+ZsQxfH3dKND3QAAVh5IKPpkm/u0eXyyLsDf0+wfnBA2YtUkJT4+HoDQ0KLV26GhoZbnrjR16lT8/Pwsj4iI8n0pCGEVu+bBuf3g7gc9JpS7mJX7C5t6mkpTj7CemwqbDs39nSwMLpf6Tm2aBalx9g1MCBtx+OieSZMmkZKSYnmcOnXK0SGJ6iovC9a8qW3f8Ax4BparmOy8AtYXTrolSYqwpr6F76dNx5NIzc4r+mTjQZcWH1w71QHRCWF9Vk1SwsLCAEhIKFoVmZCQYHnuSkajEV9f3yIPIRxi82eQegZ8a0OnR8tdzKbjSWTmFhDqa6RFLXk/C+uJrOFFVLAX+SbFusI1oSx0Orhpsra983s4d8D+AQphZVZNUiIjIwkLC2PVqlWWfampqWzevJmuXbta81JCWFdGEmz4UNu+8UVwdS93Ueb+An2bhsoss8LqzLVzq67slwJQpzM0GQzKdGmEmhCVWJmTlPT0dKKjo4mOjga0zrLR0dGcPHkSnU7HU089xRtvvMGvv/7Knj17uP/++wkPD2fYsGFWDl0IK1r/LuSkQFhLaDWy3MUopSz9BW6Sph5hA+Yh7asPniOvoJhZZvu9DnoXOLIcjq+zc3RCWFeZk5Rt27bRtm1b2rZtC8CECRNo27Ytr7zyCgDPPfccTzzxBGPHjqVjx46kp6ezbNky3N3L/5epEDaVdAy2fqFt3zQF9OWfeG1fXCpnU7LxcDXQNSrISgEKcUm7OgEEeLqSmp3PttiLVx9QowF0eEjbXvGSTJcvKrUyJym9e/dGKXXV4+uvvwZAp9MxefJk4uPjyc7OZuXKlTRq1MjacQthPStfuzT9fVSfihVVWAXfo2EN3F1llllhfQa9zrLg4F/7i2nyAej1/KXp8vcssGN0QliXw0f3COFQJzfDgV9Bp7/U6bAClu3Vhtrf3Lz4juJCWEP/Ztr7a/m+eJRSVx/gVQN6PK1tr5qijVwTohKSJEVUX0pp1eEAbe+D0GYVKu5EUgYH49Mw6HX0ayqzzArb6dUoGHdXPWeSs9gXV8IEmF3+TxuplnpamztFiEpIkhRRfe1fAqe3gKsn9HmxwsUt36fVonSpH4i/p1uFyxOiJB5uBnoVLji4Yl/xE2Xi6gF9tb6C/P0RZCTaKTohrEeSFFE95edofVEAuv0HfCrePLN8n9Y/QJp6hD0MaKG9z5aVlKQAtLwTaraGnFRY+7adIhPCeiRJEdXTls/hYgx4h0K3Jypc3LnUbLaf0EZamPsLCGFLNzYOxUWv43BCOsfPpxd/kF4P/d/Qtrd9BecP2S9AIaxAkhRR/WQkwrp3te2+r4DRu8JFrigcZdEmwp8wPxluL2zPz9PVMszdXItXrMie0PgWUAWwvOLNmkLYkyQpovpZ85ZW/R3WClrfY5Uizf1RzFXwQtiDuWlx+bWafAD6TwG9Kxz9C46stENkQliHJCmieknYD9vnaNsDpmrV4RWUkpnHxmNJgPRHEfbVv1koOh1En0omPiW75AODoqBz4XpUK16Egnz7BChEBUmSIqoPpWD5C9q6Jk2HQL0eVil21cEE8k2KRqHeRNbwskqZQpRGiK877eoEALBi/3VqU3o+Cx6BcP7gpURdCCcnSYqoPo6sgONrwOBmlYnbzP6UCdyEAw0ofN/9uec6SYqHP/R5Qdte8xZkFTOlvhBORpIUUT0U5F3qNNj5MQisb5Vi07LzWHf4PAC3tKpplTKFKIuBLbUkZXNMEufTcq59cPsHIbgJZF2Ade/ZITohKkaSFFE9bP0Cko6AZw3oOdFqxa48kEBuvomoYC8ah/pYrVwhSqt2gCdtIvwxKVi29+y1Dza4wM1vattbPoPEo7YPUIgKkCRFVH0ZibBmqrZ944vg7me1on/frX0p3NIqHJ1OZ7VyhSiLwYW1eEt3XydJAWjQDxr21xbVXD7JxpEJUTGSpIiqb9VkyEmBsJbQ7gGrFZuSlcf6w9pU44OlqUc40MCW2vtvS+wFzqVeY5SP2c1vaUOSj6yAw8ttHJ0Q5SdJiqja4nbCjm+17YHvgd5gtaJX7k8gt8BEwxBvGklTj3CgWv4etKvjj1KXOnJfU42G2gKEAMv+qy0TIYQTkiRFVF0mE/zxHKCg5Qio29Wqxf++x9zUI7UowvFuaRUOXGqCvK5ez2nLQlw4Dptm2jAyIcpPkhRRde1ZULjKsRfc9LpVi07JzGPDkcJRPS0lSRGON6hwlM/WExeuPbGbmdHn0lD8de9BaimTGyHsSJIUUTXlpMFfhcvU95wIvuFWLX7F/njyChSNQ31oKE09wgnU9POgfd2AwiafUiYcLUdA7U6QlwErX7VtgEKUgyQpompa/x6kJ2jzoXQdb/Xil+6Wph7hfMy1er/tiivdCXo9DHoX0MHu+XByk+2CE6IcJEkRVc/5w7CxsI19wNvgYrRq8YnpOfx9VEb1COdzS6ua6HSw42Qypy5klu6k8LbQ7n5t+4+Jsq6PcCqSpIiqRSn44xkw5WlzQTS62eqXWLorjgKTonVtP+oHe1u9fCHKK9TXnW5RQQAsiT5T+hP7vgLu/hC/B7Z9aZvghCgHSVJE1bL3Z4hZDy7uMPBdm1xiUbRWlT6sbS2blC9ERQxro70vF+08g1KqdCd51YB+hX1SVr8BaaUYxiyEHUiSIqqO7BRtlWOAGyZCYKTVLxGTmMGuU8kY9DoGt7JuZ1whrGFAizCMLnqOnc9gX1xq6U9s9wDUag85qbDiJdsFKEQZSJIiqo41bxV2lo2C7v+xySXMVejdG9Qg2Me6fV2EsAYfd1f6NQ0FYPHOMjT56A1wyweg08OehXB8nY0iFKL0JEkRVcPZXbDlc237lvet3lkWQCnFksKmntvaSi2KcF7mpshfC/tPlVp4W+g4Rtv+/RnIz7VBdEKUniQpovIzmWDpBFAmaH47RN1ok8vsOp1CTGIGHq4G+jcLs8k1hLCGXo2C8fd05VxaDhuPJZXt5D4vgleItmr4v5/YJkAhSkmSFFH57fwWzmwDNx9t4TQbMVed39QsFC+ji82uI0RFubnoGVQ4Z8risozyAfDwh5vf1LbXvwcXY60amxBlIUmKqNzSEi7NLNtnEvjaZt6SvAITS3ebR/VIU49wfrcVNvks2xtPZm4Z5z5peSfUuwHys7Vmn9KOEhLCyiRJEZXbsue1UT0120CnR212mTUHz5GYnksNbyM9Gwbb7DpCWEuHugHUCfQkPSefZaVZGflyOh0M/ggMRji6Evb8ZJsghbgOSVJE5XXoT9i3CHQGuPUTMNiuCWbh9tMADG9XCxeD/NoI56fT6bizfW0AFm47XfYCajSEns9q28ueh4wy9m0Rwgrk01ZUTtmpWjU0aGvz1Gxts0udT8th9cFzANzZobbNriOEtQ1vXxudDjYeT+JkUimnyb9c9ychpBlkJsGKF60foBDXIUmKqJxWT4HUMxBQD3pPsumlFu88Q4FJ0SbCnwYhsuKxqDzC/T3o0aAGAD/tKEdtiosbDPkE0MGuH+HYausGKMR1SJIiKp9TW2DL/7TtwdPAzdNml1JKsWDbKQBGdIiw2XWEsJU7C9+3P28/jaksc6aYRXSETo9o20ufhtxy1MgIUU6SpIjKJT8Xfv0PoKD1PRDVx6aX23U6hSPn0nF31TO4tax4LCqf/s1C8XV34UxyFv+Wdc4Us76vgG8tbTjy2qlWjU+Ia5EkRVQuG96H8wfAs8aluRxsyFyLMrBFTXzdXW1+PSGszd3VwNDCRQfN7+cyM/poU+YDbJwOp7dbKTohrk2SFFF5nN0FGwo/KAe9B56BNr1cVm4Bv+3S5kYxj5IQojIyd/heti+elMy88hXSeCC0uEOb2Xnx/0FethUjFKJ4kqSIyiE/Fxb9H5jyodlQaHG7zS/526440rLzqRPoSZf6QTa/nhC20rKWH01r+pKbbypfB1qzQe9pU+YnHpJmH2EXkqSIymH9u3BuH3gGwaAP7HLJuZtPAHBP5zro9Tq7XFMIW9DpdNzbuQ6gva9VeWeQ9QyEIdO07X8/gdPbrBOgECWQJEU4v7idsOFDbfuWD8Db9jO+7jmdwq7TKbgZ9NLUI6qEYW1r4eVm4Pj5DDYdv1D+gprcAi1HSLOPsAtJUoRzy8+BxeNAFUDz27SHHZhrUQa2DCPI22iXawphS95GF4YVrufzfeH7u9wGvgPeoZB4GNbYvgO7qL4kSRHObd07cG6/Nppn0Pt2uWRqdh5LorUOs/d2rmuXawphD+b38/K98ZxPyyl/QZ6BMORjbXvjdG3uIiFsQJIU4bxObIS/P9K2B38IXjXsctlFO86QlVdAo1BvOtYLsMs1hbCHZuG+tK3jT75JlX84slnjgdDqLq3Z55exkJNmnSCFuIwkKcI5ZafCorHaB2Cru7QRPXaglLI09dzbuS46nXSYFVXLfYW1KT9sPklBeWagvdzAd8AvAi7GwLL/WiE6IYqSJEU4pz+fg+ST4F9HG/ZoJ5tjLnA4IR0PVwO3tatlt+sKYS+3tKqJv6crZ5KzLAtnlpuHP9w2G9DBzu9h/6/WCFEIC0lShPPZ+4u2mJlOD7f/D9x97XbpL/+OAeD2drVkhllRJbm7GhjZUVvP56vC93uF1OsBPZ7Stn/7D6TGVbxMIQpJkiKcS8ppWPqUtn3DM1Cni90ufSIpg5UHEgB4sHuk3a4rhL090LUeBr2OjceT2B+XWvECe78ANVtD1kVtWLLJVPEyhUCSFOFMTCZY9Bhkp0Ct9tDrebtefs4/sSgFvRsH0yDE267XFsKewv09GNgiDICv/rFCbYqLG9z+Bbh4wPG1sHl2xcsUAklShDP5ZxrEbgBXT62Zx2C/5pbU7DwWFo52eEhqUUQ18FAP7X3+a3RcxYYjmwU3urTo58pXIS664mWKak+SFOEcTvwLq9/Qtge+C0FRdr38gq2nyMgtoGGINzc0tM9QZyEcqV2dANrW8Se3wMT3myo4uZtZh4eg8S1QkAsLR2uj9ISoAKsnKa+99ho6na7Io0mTJta+jKhKMhLhp4e1WWVb3QVt77Pr5fMLTMz5JxbQ/rqUYceiujDXGs7dfILsvIKKF6jTwdDp4FdHG5b823+gvOsECYGNalKaN2/O2bNnLY+///7bFpcRVYGpcCKotDio0Uhbm8fOScLyfQmcSc4iwNOV29rKsGNRfQxsEUa4nzuJ6bks3nnGOoV6BsKdc0DvAvsWwbYvrVOuqJZskqS4uLgQFhZmedSoIdXnogT/fATHVmkd7u78Boz27bCqlGLm2qMAjOpaD3dXg12vL4QjuRj0lr4pn60/XvHJ3cxqd4CbJmvbyybB2V3WKVdUOzZJUo4cOUJ4eDj169fn3nvv5eTJkyUem5OTQ2pqapGHqCYu74cy6D0IbWb3ENYdPs++uFQ83Qw82K2e3a8vhKPd3akO/p6uxCRm8Ofes9YruMs4aDxI65+y4AHpnyLKxepJSufOnfn6669ZtmwZs2bNIiYmhhtuuIG0tOLXdZg6dSp+fn6WR0REhLVDEs4oLQF+eujStPd27odiNnPNMQDu6VSHAC83h8QghCN5GV0YXZigz1hzDGWtPiQ6HQydcal/ypJx0j9FlJlOWe0dWbzk5GTq1q3Lhx9+yMMPP3zV8zk5OeTkXBr+lpqaSkREBCkpKfj62m+mUWFH+bnwzRA4tQlqNIZHVtu9mQdga+wF7py9ETeDnvXP9SHMz93uMQjhDJIzc+n29moycwuYM7ojfZqEWK/w09tgzkCtRuXGl6HnROuVLZxKamoqfn5+Vv3+tvkQZH9/fxo1asTRo0eLfd5oNOLr61vkIaq4Zc9rCYrRD+7+0SEJCsDMNdp7cnj7WpKgiGrN39ON+7poCw9OX3PUerUpoPVPGfS+tr36DTi8wnpliyrP5klKeno6x44do2bNmra+lKgMtn8D274CdDD8f3afD8VsX1wKaw6dR6+DR3s6JgYhnMmYHpG4GfRsP3GRzTEXrFt4+weg/YOAgp/HQNIx65YvqiyrJykTJ05k3bp1xMbG8u+//3LbbbdhMBi4++67rX0pUdmc2gp/FFb19nkRGt3ssFCmrTwCwC2twqlXw8thcQjhLEJ83bmzQ20APi78/bCqge9CRGfISYF590BO8f0Uhbic1ZOU06dPc/fdd9O4cWNGjBhBUFAQmzZtIjg42NqXEpVJWgIsGKW1SzcZrC0e6CC7TiXz1/4E9Dp4sm9Dh8UhhLMZ16cBbgY9G48n8c/RROsW7uIGI74Fn5pw/qAsRChKxepJyrx584iLiyMnJ4fTp08zb948oqKkOr1ay82EH++CtLMQ3ARumw16x63I8P6KQwDc1ra2LCQoxGVq+XtwT+c6gPZ7YvVxFT5hMOI7MLjBgd9gzZvWLV9UObJ2j7AtkwkWjYW4HeARCHf9AEYfh4Wz+XgSG44k4qLXSS2KEMUY1ycKd1c9O08ms+bQOetfIKIjDJ6mbW94H3Z+b/1riCpDkhRhWytf1f5iMrhpCYqDOsqCNrvsBysOAzCyYwR1gjwdFosQzirEx50HCudNeX/5YUzWmoX2cm3vhRsK+6f99iQcX2f9a4gqQZIUYTvbvoJ/P9G2h86Eul0dGs6GI4lsib2Am4uex29s4NBYhHBmj/WMwtvowv6zqfy5N942F+nzIjS/HUz5Wn+184dscx1RqUmSImzj6Er4/bKRPK3udGg4BSbF1D8PAnBf57rU9PNwaDxCOLMALzceLlzT573lB8nNt0EHV70ehs3SRvxkp8DcOyH9vPWvIyo1SVKE9cVFw4LRoAqg9d3Q81lHR8RP209x4GwqPu4uUosiRCk80rM+NbyNxCZl8u3GWNtcxNVdawYOqAfJJ+DHkZCTbptriUpJkhRhXYlH4fvhkJsG9W6AIR9ra3g4UHpOPu8X9kV5sm9DAmWNHiGuy9vowsT+jQD4ZNURLmbk2uZCXjXgnoXgEQBntsP8eyE/5/rniWpBkhRhPSln4LthkJkINVtrfyG5GB0dFbPXHuN8Wg71gjy5v2s9R4cjRKVxZ4cImtb0JTU7n2krD9vuQsGN4N6fwNULjq+FXx4BU4HtricqDUlShHVkJMF3t0HKKQhqAPf+DO6OX4fpTHIW/9twHID/DmyKm4u85YUoLYNex8u3NAXg+80nOXrOhrPE1u4Ad30PelfYvwSWPi2rJgtJUoQV5KTB3Dsg8RD41oJRi8DbOWYYnvrHAXLyTXSODOTm5qGODkeISqdbgxr0axpKgUkxZekB60/wdrmoG2H4F4AOdnwDqybb7lqiUpAkRVRMbgb8cNelydpGLQL/Oo6OCoD1h8+zdPdZ9Dp4eXAzdA7uGyNEZfXCoCa4GfSsO3zedkOSzZoPgyHTtO2/P4R179r2esKpSZIiyi83A+aOgBN/g9FXa1MObuzoqADIzivg5SV7AXigWz1a1PJzcERCVF71g715rLc2EePrv+0jLTvPthdsPxr6va5tr3lTEpVqTJIUUT5XJij3/QK12zs6KosZa45yIimTUF8jE25q5OhwhKj0xvWOol6QJwmpOZaZm22qx1PQ91VtWxKVakuSFFF2xSUoER0dHZXF0XNpzF53DIDXhjTHx93VwREJUfm5uxp4Y1hLAL7ZGMvu08m2v+gNEyRRqeYkSRFlk5Pm1AmKyaR44Ze95BUobmwSwoAWYY4OSYgqo0fDGgxtE45SMOmXPeQV2GAm2itdmaisfVtG/VQjkqSI0stIhK8HO22CAvDVPzFsib2Ap5uB129tLp1lhbCyl25php+HK/viUpm++qh9Lnp5orJ2KiybpK2wLqo8SVJE6SSfgq9uhrPR4BkE9y9xugTl6Lk03l2uLVL24i1NiQiUVY6FsLZgHyNThrUAYPqao/Zp9gEtURnwtra9eRYsfgwKbNyBVzicJCni+s4dhC/7Q9JR8IuAh5ZDrXaOjqqIvAITExbsIjffRK9GwdzTyTmGQQtRFd3aOpxbWtWkwKSYsGAX2Xl2mh22y//BbZ+D3gV2z4d590Jupn2uLRxCkhRxbae2wpwBkBYHwU20BKVGQ0dHdZWZa46x+3QKvu4uvDO8lTTzCGFjbwxtQbCPkaPn0nm/sAbTLlqPhLt+BBcPOLJcm+k684L9ri/sSpIUUbI9P8E3gyHrItTqAA/+CX61HB3VVbbFXuCT1UcAmDKsBWF+7g6OSIiqL8DLjXeGa6N9vvg7hrWHztnv4o36w/2Lwd0PTm2CL/ppi5uKKkeSFHE1pbQe9D8/DPnZ0Gig1gfFM9DRkV0lKT2Hx3/YSYFJMbRNOLe2Dnd0SEJUGzc2CeW+LlrT6tPzo4lLzrLfxet0KfzDKQIuHIMv+sLxdfa7vrALSVJEUXlZWnKydqr2c9fH4a65YPR2bFzFMJkUT82PJj41m6hgL966raU08whhZy/d0owWtXy5mJnHEz/utM+wZLPQ5vDIaqjdEbKT4fvbYdsc+11f2JwkKeKS1DhtiPHen7WOabd+Cje/CXqDoyMr1ow1R9lwJBF3Vz2z7muPl9HF0SEJUe24uxqYeU97fNxd2H7iIu/Zs38KgHcIPLAUWtwBpnxY+hT8+V8Z+VNFSJIiNMfXwuwb4Mw28AiAUYuh3f2OjqpEqw8m8NFKbWruN4a1pFGoj4MjEqL6qhPkyXt3tAbg8/XH+XVXnH0DcHXXVk/u86L28+ZZ2h9cqXaOQ1idJCnVnckE696Db4dBZiKEtoQxqyDyBkdHVqKD8ak88cNOTAru7lSHO9rXdnRIQlR7A1qE8WjP+gBMXLiLHScv2jcAnQ56PQcjvtMmmzy1CT7rKf1UKjlJUqqzzAvwwwhY8wagoO0oGPMXBEU5OrISnU/L4eGvt5GRW0C3qCAmD23u6JCEEIWeG9CEfk1DyM03MfbbbZy+6IA5TJrdCmPXQmgLyDgP3w2D9e/JDLWVlCQp1dWxNTCrOxz9C1zcYegMGDodXD0cHVmJsvMKGPvdNs4kZxFZw4uZ97bD1SBvYSGchUGv4+O72tIkzIfE9FzGfLONtGwH9A0JioIxK6HNfaBMsPoN+G4opJy2fyyiQuQTvrrJy4I/n9f+ukiLg8AoePgvaHufoyO7ptx8E+Pm7mDnyWT8PFz58oEO+Hu6OTosIcQVvIwufDm6IzW8jRyMT2PMN9vsNyPt5Vw9YNgMuHW6NvFbzHqY2Q12L5QFCisRSVKqk7idWhvt5tnazx0ehsc2QM1Wjo3rOrSpt6NZffAc7q56/nd/B+oHO9+QaCGEppa/B3NGd8Tb6MLmmAuMm7uD3HwHNbe0GwWP/Q212kNOCvwyBn56SGaprSQkSakO8rJg1eTCWRkPg3co3PsTDP4Q3LwcHd01KaV4cdEelu4+i6tBx+z72tMp0vkmlRNCFNWyth9fPtABo4ue1QfPMWFBNAUmB9Vg1GgAD62A3i+AzgD7foGZXWHfYqlVcXKSpFR1x9Zov4wbPtDmEGg2FMZtgoY3OTqy6zKZFC8u3su8rafQ6+Dju9rSu3GIo8MSQpRS5/pBfDaqPa4GHUt3n2XCgmj7TvZ2OYML9H6+cHBAQ0iPh4UPwI93aau8C6ckSUpVlZEIv4zV+p5cjAGfcBg5F0Z865TT219JW9U4mh82n0Sng3eGt2JQy5qODksIUUa9G4fwyV1tcdHrWBIdx7i5O8jJd0AfFbNa7bXmn57Pgd4VDi+DGZ1h4wwoyHdcXKJYOqWcq64rNTUVPz8/UlJS8PX1dXQ4lU9+Lmz5HNa9q7W/ooNOY+HGl8C9ctzPnPwCHv9hJ3/tT8BFr+PDkW1kTR4hKrmV+xMY94PWN+WGhjX4bFR7PN0cPEv0uYPaDLUnN2o/hzSD/m9Ag74ODauyssX3tyQpVYVScHAprHhZqzkBCGsJgz+G2u0dG1sZXMjI5bHvtrMl9gJuLnpm3tOOfs1CHR2WEMIK/j2ayJhvt5GZW0Cr2n787/4OhPo6eNVykwl2fgcrX9VWfAdo2F9LVoIbOza2SkaSFFG8U1tg5etw4m/tZ+9QuPFlaHOP0667U5wjCWk89M1WTl3Iwsfowmej2tOtQQ1HhyWEsKIdJy/y8NdbuZiZR5ivO1880IEWtfwcHZY22mf9+7DlM63/ns4AHR6EGyaCrzQ1l4YkKaKo09tgzVtwbJX2s4s7dPsPdH/SKVctvpbVBxN48sdo0nLyqRPoyZcPdKChrMcjRJV0IimDh7/ZxtFz6Xi4Gnj/ztbc0spJEoGkY/DXK1rNNGifq+0fhB5Pg4/U6l6LJClCc3o7rHsbjqzQftYZtFqT3v8Fv8q1jk1egYn3lx/is/XHAehUL5DZo9oT6CUTtQlRlaVm5/H4DztZf/g8APd3rcsLg5ri7uoktb+xf2sz1Zr7q7i4a3NLdX9SkpUSSJJSnZlMWi/0jdPhxD/aPp0BWt8NPSdCYKRj4yuHUxcy+c+8new8mQzAA13r8sItTTG6OMmHlBDCpvILTLy/4jCz1x0DoFlNX6bf09Z5JmtUCo6vgTVT4fQWbZ/BDVqOgK7jIbSZY+NzMpKkVEe5GbBrHmyaCUlHtX16V2g1Am54xqkXAyyJyaSYu/kEb/95kIzcAnzdXXj3jlYMaOEk1b1CCLtae+gczyzYRVJGLu6uep65qTEP9YjEoNc5OjSNUlqz+rr3tNWVzaL6QrfHIbI36GVGD0lSqpP4PbD9a9i9AHJStX3uftDhIW1IsW/lHJIbk5jB8z/vZkuMNiV1+7oBTBvZhohATwdHJoRwpITUbJ5ZsIu/jyYC0DrCn3eHt6JxmJP1TTu1FTZ+Cgd+0xYvBAisD+0egDb3gnewY+NzIElSqrqsZNi/GHZ8C2e2X9ofEAmdH9MWAaxkHWLN0rLzmL7mKHP+jiW3wISnm4Hnbm7MqK71nOevJSGEQymlmL/1FG/+foC0nHwMeh2jutTlqX4NnW9B0QsxsGkWRP8AuWnaPr0rNLkF2o6C+r21WW6rEUlSqqK8LDi8HPYs1DrCFuRq+/Wu0HQwtB8N9XpW2qrEvAITC7ed5sO/DpGYrr22no2CeXNYC6k9EUIUKz4lm1eW7GXF/gQA/DxceapfQ+7pXMf5+qzlpMO+RVrN95ltl/Z7BUPz27T+K7U7gK7q/zEmSUpVkZ0KR1fCoT+0BMXcnAMQ0hxaj4TW91TqasO8AhM/bz/N9DVHOX0xC4D6Nbx48Zam3NgkBF01+IUVQlTM30cSmbJ0P4cStJqKmn7u/F/vKEZ0iHCeUUCXi9+j1YTv/Rkyky7t968LTYdA40FQp0ulmr+qLCRJqcwuxGgdrw79CTHrL9WYAPhFQMs7oOWdENrccTFaQUpWHgu3nWLOP7GcSdaSkxrebvxf7waM6lIXN5fKWSMkhHCM/AIT87ed4tNVR4lPzQYg1NfI/V3rcU+nOgQ443QFBXlwfK1WQ35gKeRlXHrOMwgaDYBGN0NkL/Dwd1SUVidJSmWSdVEbZ39stbYSsXmqerOgBlpW3WQw1O5YaZtzzPbFpTBvyyl+3nGazFxt8bAa3kYe61WfezvXxcOtav7lIISwj+y8AhZuO8WMNccsyYrRRc9tbWtxV6c6tK7t55w1tLkZWs35wT+0aSSyky89p9NrCx5G3Qj1+2jbLk6YdJWSJCnOSilIOQ0nN8HJf7V/z+0veozeBWp3gob9oMkQCG7kmFitKD4lm6W74/h5xxkOnL3UZNUo1JsHu0cyrE0tSU6EEFaVk1/Ab7vOMuefGPbFXfrcaRDizR3tazO4VU1qBzhpf7eCPG1yuIN/aDXriYeLPu/iDrU6aE1Cdbtqf8C6O8GSAaUkSYozUArSz8HZXRC3A87sgLidkHHu6mNrNNKy46g+UK8HGJ1sKF0ZKaU4dj6dVQfOsWxfvGUSNgA3g56bmoVyT+c6dIsKcs6/aIQQVYZSim0nLjJ30wmW7YsnO89kea5lLT8GtAijb9MQGof6OO/nUcpprab92GqIWVe0H4tZUEOo1Q7C20J4O61LgJOO8pQkxd4yL0DiES3bPbcfEvZCwn7ITLz6WJ0BaraCOt20LLhO10rd8dXsbEoWW2Mv8u/RRDYcSbT0MzFrXzeAYW1rMaRVTecbIiiEqBZSs/P4Y/dZftl5hq2xF7j8Wy3U18gNDYPp3iCIjvUCnbeWRSnt++bkRu1x4l9IPlH8sQH1ILQFhDTTZr0NaqhN7OnqYdeQryRJirWZTJBxXnsjXIy99Eg6BklHis9qQWtHDIwqzG7baf+GtXT4G8RaLmTkMmXpfrbGXrCMzDFzM+jpFBnIzc1D6d88zPHLrAshxGXOp+Xw1/4Elu+LZ9PxJHLyTUWeD/dzp0O9QCYNakJNPyf/zE4/r9XUx+3Uau7jdkJ6QgkH68A/QktYAutDQF0tmQmop40ucrf9H/2VKkmZMWMG7733HvHx8bRu3ZpPP/2UTp06Xfc8myUpKWdgy+eQGgepZ7RqtrSzRUfZFMe3NtRoUJixNtf+DW4Cbk6ajVtBbr6Jlq8tJyffhF4HzcJ96VQviBsa1aBLZJD0MxFCVArZeQVsjb3AhiOJbI65wL4zKeSbFHod7Hq1Pz7uro4OsewyEiFh36Xa/fOHtBqYyzvkFsfoq81U7lsL/GppiUyPp60aWqVJUubPn8/999/P7Nmz6dy5M9OmTWPhwoUcOnSIkJCQa55rsyTl/GGY0fHq/Tq9logE1L0s84yEGg21EThuXtaLoRKZv/Uk4f4etK0TgLexes2aKISomjJz84k+mcyxxAxGdanr6HCsRymt5j/xsJawXIwt2kJQbF+XBvDE9qv3V0ClSVI6d+5Mx44dmT59OgAmk4mIiAieeOIJ/vvf/17zXJslKbkZsPJ1LYP0rQV+tbWs0qcmGCphNi2EEEKURm6G1pqQWvhIOaP9Ad7tcatexhbf31b/Ezk3N5ft27czadIkyz69Xk+/fv3YuHHjVcfn5OSQk5Nj+Tk1NfWqY6zCzQsGvWubsoUQQghn5ealTXtRCae+sPoMYomJiRQUFBAaGlpkf2hoKPHx8VcdP3XqVPz8/CyPiIgIa4ckhBBCiErI4dOcTpo0iZSUFMvj1KlTjg5JCCGEEE7A6s09NWrUwGAwkJBQdJhUQkICYWFhVx1vNBoxGo3WDkMIIYQQlZzVa1Lc3Nxo3749q1atsuwzmUysWrWKrl27WvtyQgghhKiibDK2dMKECTzwwAN06NCBTp06MW3aNDIyMnjwwQdtcTkhhBBCVEE2SVJGjhzJ+fPneeWVV4iPj6dNmzYsW7bsqs60QgghhBAlqd7T4gshhBDCKmzx/e3w0T1CCCGEEMWRJEUIIYQQTkmSFCGEEEI4JUlShBBCCOGUJEkRQgghhFOSJEUIIYQQTskm86RUhHlEtM1WQxZCCCGE1Zm/t605s4nTJSlpaWkAshqyEEIIUQmlpaXh5+dnlbKcbjI3k8lEXFwcPj4+6HQ6R4dTJqmpqURERHDq1CmZiO4a5D6Vjtyn0pH7VDpyn0pH7lPpFHeflFKkpaURHh6OXm+d3iROV5Oi1+upXbu2o8OoEF9fX3lzl4Lcp9KR+1Q6cp9KR+5T6ch9Kp0r75O1alDMpOOsEEIIIZySJClCCCGEcEqSpFiR0Wjk1VdfxWg0OjoUpyb3qXTkPpWO3KfSkftUOnKfSsde98npOs4KIYQQQoDUpAghhBDCSUmSIoQQQginJEmKEEIIIZySJClCCCGEcEqSpAghhBDCKUmSch0zZsygXr16uLu707lzZ7Zs2VLisb1790an0131uOWWWyzHKKV45ZVXqFmzJh4eHvTr148jR47Y46XYlDXvU15eHs8//zwtW7bEy8uL8PBw7r//fuLi4uz1cmzG2u+nyz322GPodDqmTZtmo+jtxxb36cCBA9x66634+fnh5eVFx44dOXnypK1fik1Z+z6lp6fz+OOPU7t2bTw8PGjWrBmzZ8+2x0uxqbLcJ4Bp06bRuHFjPDw8iIiI4OmnnyY7O7tCZVYG1r5PU6dOpWPHjvj4+BASEsKwYcM4dOhQ2YJSokTz5s1Tbm5u6quvvlL79u1TjzzyiPL391cJCQnFHp+UlKTOnj1reezdu1cZDAY1Z84cyzFvv/228vPzU4sXL1a7du1St956q4qMjFRZWVl2elXWZ+37lJycrPr166fmz5+vDh48qDZu3Kg6deqk2rdvb8dXZX22eD+Z/fLLL6p169YqPDxcffTRR7Z9ITZmi/t09OhRFRgYqJ599lm1Y8cOdfToUbVkyZISy6wMbHGfHnnkERUVFaXWrFmjYmJi1GeffaYMBoNasmSJnV6V9ZX1Ps2dO1cZjUY1d+5cFRMTo5YvX65q1qypnn766XKXWRnY4j7dfPPNas6cOWrv3r0qOjpaDRo0SNWpU0elp6eXOi5JUq6hU6dOavz48ZafCwoKVHh4uJo6dWqpzv/oo4+Uj4+P5T/EZDKpsLAw9d5771mOSU5OVkajUf3444/WDd6OrH2firNlyxYFqBMnTlQ4Xkex1X06ffq0qlWrltq7d6+qW7dupU9SbHGfRo4cqe677z6rx+pItrhPzZs3V5MnTy5yXLt27dSLL75onaAdoKz3afz48erGG28ssm/ChAmqe/fu5S6zMrDFfbrSuXPnFKDWrVtX6rikuacEubm5bN++nX79+ln26fV6+vXrx8aNG0tVxpdffsldd92Fl5cXADExMcTHxxcp08/Pj86dO5e6TGdji/tUnJSUFHQ6Hf7+/hUN2SFsdZ9MJhOjRo3i2WefpXnz5laP295scZ9MJhO///47jRo14uabbyYkJITOnTuzePFiW7wEu7DV+6lbt278+uuvnDlzBqUUa9as4fDhw/Tv39/qr8EeynOfunXrxvbt2y1NHcePH+ePP/5g0KBB5S7T2dniPhUnJSUFgMDAwFLHJklKCRITEykoKCA0NLTI/tDQUOLj4697/pYtW9i7dy9jxoyx7DOfV94ynZEt7tOVsrOzef7557n77rsr7aqktrpP77zzDi4uLvznP/+xaryOYov7dO7cOdLT03n77bcZMGAAK1as4LbbbuP2229n3bp1Vn8N9mCr99Onn35Ks2bNqF27Nm5ubgwYMIAZM2bQs2dPq8ZvL+W5T/fccw+TJ0+mR48euLq6EhUVRe/evXnhhRfKXaazs8V9upLJZOKpp56ie/futGjRotSxSZJiI19++SUtW7akU6dOjg7FqV3vPuXl5TFixAiUUsyaNcvO0TmP4u7T9u3b+fjjj/n666/R6XQOjM55FHefTCYTAEOHDuXpp5+mTZs2/Pe//2Xw4MFVolNoeZT0e/fpp5+yadMmfv31V7Zv384HH3zA+PHjWblypYMitb+1a9fy1ltvMXPmTHbs2MEvv/zC77//zpQpUxwdmlMp630aP348e/fuZd68eWW6jos1gq2KatSogcFgICEhocj+hIQEwsLCrnluRkYG8+bNY/LkyUX2m89LSEigZs2aRcps06aNdQK3M1vcJzNzgnLixAlWr15daWtRwDb3acOGDZw7d446depY9hUUFPDMM88wbdo0YmNjrRa/vdjiPtWoUQMXFxeaNWtWZH/Tpk35+++/rRO4ndniPmVlZfHCCy+waNEiy4ifVq1aER0dzfvvv1+kKaCyKM99evnllxk1apSllqlly5ZkZGQwduxYXnzxxQrde2dli/uk11+qA3n88cdZunQp69evp3bt2mWKTWpSSuDm5kb79u1ZtWqVZZ/JZGLVqlV07dr1mucuXLiQnJwc7rvvviL7IyMjCQsLK1Jmamoqmzdvvm6ZzsoW9wkuJShHjhxh5cqVBAUFWT12e7LFfRo1ahS7d+8mOjra8ggPD+fZZ59l+fLlNnkdtmaL++Tm5kbHjh2vGvp4+PBh6tata73g7cgW9ykvL4+8vLwiXy4ABoPBUhtV2ZTnPmVmZhZ7D0CbQqIi995Z2eI+mf99/PHHWbRoEatXryYyMrLswZW6i201NG/ePGU0GtXXX3+t9u/fr8aOHav8/f1VfHy8UkqpUaNGqf/+979XndejRw81cuTIYst8++23lb+/v1qyZInavXu3Gjp0aJUYgmzN+5Sbm6tuvfVWVbt2bRUdHV1k2GROTo7NX4+t2OL9dKWqMLrHFvfpl19+Ua6ururzzz9XR44cUZ9++qkyGAxqw4YNNn0ttmSL+9SrVy/VvHlztWbNGnX8+HE1Z84c5e7urmbOnGnT12JLZb1Pr776qvLx8VE//vijOn78uFqxYoWKiopSI0aMKHWZlZEt7tP//d//KT8/P7V27doin+OZmZmljkuSlOv49NNPVZ06dZSbm5vq1KmT2rRpk+W5Xr16qQceeKDI8QcPHlSAWrFiRbHlmUwm9fLLL6vQ0FBlNBpV37591aFDh2z5EuzCmvcpJiZGAcU+1qxZY+NXYlvWfj9dqSokKUrZ5j59+eWXqkGDBsrd3V21bt1aLV682Fbh242179PZs2fV6NGjVXh4uHJ3d1eNGzdWH3zwgTKZTLZ8GTZXlvuUl5enXnvtNRUVFaXc3d1VRESEGjdunLp48WKpy6ysrH2fSvocL26up5LoCgsSQgghhHAq0idFCCGEEE5JkhQhhBBCOCVJUoQQQgjhlCRJEUIIIYRTkiRFCCGEEE5JkhQhhBBCOCVJUoQQQgjhlCRJEUIIIYRTkiRFCCGEEE5JkhQhhBBCOCVJUoQQQgjhlP4fETRgso+bKV8AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This comparison only makes sense if using ~100 epochs during training\n",
    "val_auc_lb = 0.8232\n",
    "val_auc_lb_std = 0.0090\n",
    "test_auc_lb = 0.7558\n",
    "test_auc_lb_std = 0.0140\n",
    "\n",
    "norm_plot([\n",
    "    (test_auc_lb, test_auc_lb_std, \"GIN L.B.\"),\n",
    "    (0.7544, 0.0203, 'GIN local'),\n",
    "], 'Test ROC AUC Comparison over 10 runs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# further reading\n",
    "# https://arxiv.org/abs/1704.01212"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
