{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Training of GNN with Multiple GPUs\n",
    "\n",
    "This tutorial shows how to train a multi-layer GraphSAGE for node classification on Amazon Copurchase Network provided by OGB with multiple GPUs.  The dataset contains 2.4 million nodes and 61 million edges, hence not fitting a single GPU.\n",
    "\n",
    "This tutorials' content include\n",
    "\n",
    "* Training a GNN model with a single machine with multiple GPUs on a graph of any size with `torch.nn.parallel.DistributedDataParallel`.\n",
    "\n",
    "PyTorch `DistributedDataParallel` (or DDP in short) is a common solution for multi-GPU training.  It is easy to combine DGL with PyTorch DDP, as you do the same thing as that in any ordinary PyTorch applications:\n",
    "\n",
    "* Divide the data to each GPU.\n",
    "* Distribute the model parameters using PyTorch DDP.\n",
    "* Customize your neighborhood sampling strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import dgl\n",
    "import torch\n",
    "import dgl.nn as dglnn\n",
    "import torch.nn as nn\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "import torch.nn.functional as F\n",
    "import torch.multiprocessing as mp\n",
    "import sklearn.metrics\n",
    "import tqdm\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "\n",
    "The following code is copied from the first tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    import pickle\n",
    "\n",
    "    with open('data.pkl', 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    graph, node_features, node_labels, train_nids, valid_nids, test_nids = data\n",
    "    utils.prepare_mp(graph)\n",
    "    \n",
    "    num_features = node_features.shape[1]\n",
    "    num_classes = (node_labels.max() + 1).item()\n",
    "    \n",
    "    return graph, node_features, node_labels, train_nids, valid_nids, test_nids, num_features, num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customize Neighborhood Sampling\n",
    "\n",
    "Previously we have seen how to use `NodeDataLoader` together with `MultiLayerNeighborSampler`.  In fact, you can replace `MultiLayerNeighborSampler` with your own sampling strategy.\n",
    "\n",
    "The customization is simple.  For each GNN layer, you only need to specify the edges involved in the message passing as a graph.  Such a graph will have the same nodes as the original graph.  For example, here is how `MultiLayerNeighborSampler` is implemented:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerNeighborSampler(dgl.dataloading.BlockSampler):\n",
    "    def __init__(self, fanouts):\n",
    "        super().__init__(len(fanouts), return_eids=False)\n",
    "        self.fanouts = fanouts\n",
    "        \n",
    "    def sample_frontier(self, layer_id, g, seed_nodes):\n",
    "        fanout = self.fanouts[layer_id]\n",
    "        return dgl.sampling.sample_neighbors(g, seed_nodes, fanout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Data Loader for Distributed Data Parallel (DDP)\n",
    "\n",
    "In PyTorch DDP each worker process is assigned an integer *rank*.  The rank would indicate which partition of the dataset the worker process will handle.  So the only difference between single GPU and multiple GPU training in terms of data loader is that the data loader will only iterate over a partition of the nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(rank, world_size, graph, nids):\n",
    "    partition_size = len(nids) // world_size\n",
    "    partition_offset = partition_size * rank\n",
    "    nids = nids[partition_offset:partition_offset+partition_size]\n",
    "    \n",
    "    sampler = MultiLayerNeighborSampler([4, 4, 4])\n",
    "    dataloader = dgl.dataloading.NodeDataLoader(\n",
    "        graph, nids, sampler,\n",
    "        batch_size=1024,\n",
    "        shuffle=True,\n",
    "        drop_last=False,\n",
    "        num_workers=0\n",
    "    )\n",
    "    \n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Model\n",
    "\n",
    "The model implementation will be exactly the same as what you have seen in the first tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAGE(nn.Module):\n",
    "    def __init__(self, in_feats, n_hidden, n_classes, n_layers):\n",
    "        super().__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_classes = n_classes\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(dglnn.SAGEConv(in_feats, n_hidden, 'mean'))\n",
    "        for i in range(1, n_layers - 1):\n",
    "            self.layers.append(dglnn.SAGEConv(n_hidden, n_hidden, 'mean'))\n",
    "        self.layers.append(dglnn.SAGEConv(n_hidden, n_classes, 'mean'))\n",
    "        \n",
    "    def forward(self, bipartites, x):\n",
    "        for l, (layer, bipartite) in enumerate(zip(self.layers, bipartites)):\n",
    "            x = layer(bipartite, x)\n",
    "            if l != self.n_layers - 1:\n",
    "                x = F.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributing the Model to GPUs\n",
    "\n",
    "PyTorch DDP manages the distribution of models and synchronization of the gradients for you.  In DGL, you can benefit from PyTorch DDP as well by simply wrapping the model with `torch.nn.parallel.DistributedDataParallel`.\n",
    "\n",
    "The recommended way to distribute training is to have one training process per GPU, so during model instantiation we also specify the process rank, which is equal to the GPU ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(rank, in_feats, n_hidden, n_classes, n_layers):\n",
    "    model = SAGE(in_feats, n_hidden, n_classes, n_layers).to(rank)\n",
    "    return DistributedDataParallel(model, device_ids=[rank], output_device=rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Training Loop for one Process\n",
    "\n",
    "The training loop looks the same as other PyTorch DDP applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@utils.fix_openmp\n",
    "def train(rank, world_size, data):\n",
    "    # data is the output of load_data\n",
    "    torch.distributed.init_process_group(\n",
    "        backend='nccl',\n",
    "        init_method='tcp://127.0.0.1:12345',\n",
    "        world_size=world_size,\n",
    "        rank=rank)\n",
    "    torch.cuda.set_device(rank)\n",
    "    \n",
    "    graph, node_features, node_labels, train_nids, valid_nids, test_nids, num_features, num_classes = data\n",
    "    \n",
    "    train_dataloader = create_dataloader(rank, world_size, graph, train_nids)\n",
    "    # We only use one worker for validation\n",
    "    valid_dataloader = create_dataloader(0, 1, graph, valid_nids)\n",
    "    \n",
    "    model = init_model(rank, num_features, 128, num_classes, 3)\n",
    "    opt = torch.optim.Adam(model.parameters())\n",
    "    torch.distributed.barrier()\n",
    "    \n",
    "    best_accuracy = 0\n",
    "    best_model_path = 'model.pt'\n",
    "    for epoch in range(10):\n",
    "        model.train()\n",
    "\n",
    "        for step, (input_nodes, output_nodes, bipartites) in enumerate(train_dataloader):\n",
    "            bipartites = [b.to(rank) for b in bipartites]\n",
    "            inputs = node_features[input_nodes].cuda()\n",
    "            labels = node_labels[output_nodes].cuda()\n",
    "            predictions = model(bipartites, inputs)\n",
    "\n",
    "            loss = F.cross_entropy(predictions, labels)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            accuracy = sklearn.metrics.accuracy_score(labels.cpu().numpy(), predictions.argmax(1).detach().cpu().numpy())\n",
    "\n",
    "            if rank == 0 and step % 10 == 0:\n",
    "                print('Epoch {:05d} Step {:05d} Loss {:.04f}'.format(epoch, step, loss.item()))\n",
    "\n",
    "        torch.distributed.barrier()\n",
    "        \n",
    "        if rank == 0:\n",
    "            model.eval()\n",
    "            predictions = []\n",
    "            labels = []\n",
    "            with torch.no_grad():\n",
    "                for input_nodes, output_nodes, bipartites in valid_dataloader:\n",
    "                    bipartites = [b.to(rank) for b in bipartites]\n",
    "                    inputs = node_features[input_nodes].cuda()\n",
    "                    labels.append(node_labels[output_nodes].numpy())\n",
    "                    predictions.append(model.module(bipartites, inputs).argmax(1).cpu().numpy())\n",
    "                predictions = np.concatenate(predictions)\n",
    "                labels = np.concatenate(labels)\n",
    "                accuracy = sklearn.metrics.accuracy_score(labels, predictions)\n",
    "                print('Epoch {} Validation Accuracy {}'.format(epoch, accuracy))\n",
    "                if best_accuracy < accuracy:\n",
    "                    best_accuracy = accuracy\n",
    "                    torch.save(model.module.state_dict(), best_model_path)\n",
    "                    \n",
    "        torch.distributed.barrier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00000 Step 00000 Loss 5.7553\n",
      "Epoch 00000 Step 00010 Loss 2.6858\n",
      "Epoch 00000 Step 00020 Loss 2.1455\n",
      "Epoch 00000 Step 00030 Loss 1.7148\n",
      "Epoch 00000 Step 00040 Loss 1.6470\n",
      "Epoch 0 Validation Accuracy 0.7247158151717824\n",
      "Epoch 00001 Step 00000 Loss 1.3390\n",
      "Epoch 00001 Step 00010 Loss 1.3108\n",
      "Epoch 00001 Step 00020 Loss 1.3176\n",
      "Epoch 00001 Step 00030 Loss 1.4312\n",
      "Epoch 00001 Step 00040 Loss 1.1797\n",
      "Epoch 1 Validation Accuracy 0.7972687739999491\n",
      "Epoch 00002 Step 00000 Loss 1.0574\n",
      "Epoch 00002 Step 00010 Loss 1.1461\n",
      "Epoch 00002 Step 00020 Loss 1.0746\n",
      "Epoch 00002 Step 00030 Loss 1.0027\n",
      "Epoch 00002 Step 00040 Loss 0.9308\n",
      "Epoch 2 Validation Accuracy 0.8152480736464665\n",
      "Epoch 00003 Step 00000 Loss 0.9768\n",
      "Epoch 00003 Step 00010 Loss 1.0767\n",
      "Epoch 00003 Step 00020 Loss 0.9237\n",
      "Epoch 00003 Step 00030 Loss 1.0979\n",
      "Epoch 00003 Step 00040 Loss 0.8528\n",
      "Epoch 3 Validation Accuracy 0.83111664928922\n",
      "Epoch 00004 Step 00000 Loss 0.9134\n",
      "Epoch 00004 Step 00010 Loss 0.9284\n",
      "Epoch 00004 Step 00020 Loss 0.8158\n",
      "Epoch 00004 Step 00030 Loss 0.9542\n",
      "Epoch 00004 Step 00040 Loss 0.9215\n",
      "Epoch 4 Validation Accuracy 0.839508684484907\n",
      "Epoch 00005 Step 00000 Loss 0.9607\n",
      "Epoch 00005 Step 00010 Loss 0.9081\n",
      "Epoch 00005 Step 00020 Loss 0.8607\n",
      "Epoch 00005 Step 00030 Loss 0.8400\n",
      "Epoch 00005 Step 00040 Loss 0.8883\n",
      "Epoch 5 Validation Accuracy 0.8434249675762276\n",
      "Epoch 00006 Step 00000 Loss 0.7871\n",
      "Epoch 00006 Step 00010 Loss 0.9050\n",
      "Epoch 00006 Step 00020 Loss 0.8587\n",
      "Epoch 00006 Step 00030 Loss 0.7345\n",
      "Epoch 00006 Step 00040 Loss 0.7846\n",
      "Epoch 6 Validation Accuracy 0.8497317091778348\n",
      "Epoch 00007 Step 00000 Loss 0.7165\n",
      "Epoch 00007 Step 00010 Loss 0.8370\n",
      "Epoch 00007 Step 00020 Loss 0.8072\n",
      "Epoch 00007 Step 00030 Loss 0.7852\n",
      "Epoch 00007 Step 00040 Loss 0.8651\n",
      "Epoch 7 Validation Accuracy 0.853012232027058\n",
      "Epoch 00008 Step 00000 Loss 0.8609\n",
      "Epoch 00008 Step 00010 Loss 0.6784\n",
      "Epoch 00008 Step 00020 Loss 0.7328\n",
      "Epoch 00008 Step 00030 Loss 0.8150\n",
      "Epoch 00008 Step 00040 Loss 0.8347\n",
      "Epoch 8 Validation Accuracy 0.852732497520535\n",
      "Epoch 00009 Step 00000 Loss 0.7051\n",
      "Epoch 00009 Step 00010 Loss 0.7738\n",
      "Epoch 00009 Step 00020 Loss 0.8157\n",
      "Epoch 00009 Step 00030 Loss 0.7437\n",
      "Epoch 00009 Step 00040 Loss 0.7249\n",
      "Epoch 9 Validation Accuracy 0.8549703735727182\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    procs = []\n",
    "    data = load_data()\n",
    "    for proc_id in range(4):    # 4 gpus\n",
    "        p = mp.Process(target=train, args=(proc_id, 4, data))\n",
    "        p.start()\n",
    "        procs.append(p)\n",
    "    for p in procs:\n",
    "        p.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, you have learned how to train a multi-layer GraphSAGE for node classification on a large dataset that cannot fit into GPU.  The method you have learned can scale to a graph of any size, and works on a single machine with *any number of* GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional material: caveat in training with DDP\n",
    "\n",
    "When writing DDP code, you may often find these two kinds of errors:\n",
    "\n",
    "* `Cannot re-initialize CUDA in forked subprocess`\n",
    "\n",
    "  This is because you have initialized the CUDA context before creating subprocesses using `mp.Process`.  Solutions include:\n",
    "  \n",
    "  * Remove all the code that can possibly initialize CUDA context before calling `mp.Process`.  For instance, you cannot get number of GPUs via `torch.cuda.device_count()` before calling `mp.Process` since that also initializes CUDA context.  You can check whether CUDA context is initialized via `torch.cuda.is_initialized()`.\n",
    "  \n",
    "  * Use `torch.multiprocessing.spawn()` to create processes instead of forking with `mp.Process`.  A downside is that Python will duplicate the graph storage for every process spawned this way.  Memory consumption will linearly scale up.\n",
    "  \n",
    "* Training process freezes during minibatch iteration.\n",
    "\n",
    "  This is due to a [lasting bug in the interaction between GNU OpenMP and `fork`](https://github.com/pytorch/pytorch/issues/17199).  A workaround is to wrap the target function of `mp.Process` with the decorator `utils.fix_openmp`, provided in the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
